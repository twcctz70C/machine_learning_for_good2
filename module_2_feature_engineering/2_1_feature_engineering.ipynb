{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2:  Feature Engineering\n",
    "\n",
    "Feature Engineering is the process of transforming raw data into features/input variables that are easily digested by algorithms.  New Data Scientists often spend all of their time testing out various algorithms; however, the majority of accuracy gains generally stem from well-crafted features.  In this Lab we will introduce the following types of feature engineering:\n",
    "\n",
    "1. Feature pruning\n",
    "2. Temporal Features (month, year, etc)\n",
    "3. One-hot encoding / dummy variables\n",
    "4. Extracting features from strings\n",
    "5. Metadata\n",
    "6. Feature scaling\n",
    "7. Data Imputation / cleaning\n",
    "\n",
    "\n",
    "While performing Feature Engineering, it is critical to keep in mind the question that you are trying to answer.  For the purposes of this excercise, we will be using the KIVA dataset and will be trying to answering the following question:\n",
    "\n",
    "*What drives the loan amount requested by KIVA borrowers? * \n",
    "\n",
    "In the language of Module 1, our outcome feature is **loan_amount**. In the next notebook, we will formalize this research question as a machine learning task. Our machine learning task will be to predict the loan amount that a borrower requests from KIVA using all the features we explore in this notebook.\n",
    "\n",
    "\n",
    "We may not end up using all the features we create, but the process is an important extension of exploratory analysis. The key difference between feature engineering and exploratory analysis is that we now have a defined question in mind: \"What drives the loan amount requested by KIVA lenders?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "sns.set()\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the command below tells jupyter to display up to 80 columns, this keeps everything visible\n",
    "pd.set_option('display.max_columns', 80)\n",
    "pd.set_option('expand_frame_repr', True)\n",
    "df = pd.read_csv(\"../data/raw_data.csv.zip\", low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Pruning\n",
    "No need to keep features that have zero variation.  Algorithms can only provide meaningful insights when there is variation in the features.  Given that we are performing feature engineering for the purpose of feeding these features into a machine learning algorithm, let's go ahead and remove all columns that only consist 1 or less unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].unique().size==1:\n",
    "        print(\"Dropping column: {0}\".format(col))\n",
    "        df = df.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporal Features\n",
    "Time trends are very significant and should not be neglected.  Most algorithms will not be able to make use of raw datetimes; however, will be able to find patterns in the data if they are informed which observations occur in a given year, on a weekday vs weekend, on a holiday, etc.\n",
    "\n",
    "Before we are able to extract this meta data, let's convert the strings in the pandas dataframe to datetime objects. Luckily for us, all time fields in this dataset have \"_date\" in their name.\n",
    "\n",
    "Pandas is really adept at time series, and we will use pd.to_datetime to create pandas timestamps.\n",
    "see a list of methods that can be applied to a pandas datetime. https://pandas.pydata.org/pandas-docs/version/0.21/api.html#id34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [c for c in df.columns if \"_date\" in c]:\n",
    "    if \"_date\" in col:\n",
    "        df[col] = pd.to_datetime(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .dt accessor\n",
    "Pandas .dt accessor enables you to easily construct additional features based off of these datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  posted date features\n",
    "df['posted_year']=df['posted_date'].dt.year\n",
    "df['posted_month']=df['posted_date'].dt.month\n",
    "\n",
    "## Time to fund is the funded date minus the posted date\n",
    "## we add these fields because the homework question in the next notebook involves predicting time to fund\n",
    "df['time_to_fund'] =df['funded_date'] - df['posted_date']\n",
    "df['days_to_fund'] = df['time_to_fund'].dt.days\n",
    "\n",
    "# expiration date features\n",
    "## Time to expiration is the expiration date minus the Posted Date\n",
    "df['time_to_expire_date'] =df['planned_expiration_date'] - df['posted_date']\n",
    "df['days_to_expire'] = df['time_to_expire_date'].dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-hot encoding\n",
    "One-hot encoding is the process of converting either categorical or string data into a binary. Let's practice one-hot encoding by converting the \"tags\" column into a set of binary features indicating whether or not a particular tag appears in a given row. \n",
    "\n",
    "In order to do this we will first need to convert the \"tags\" column into a list of strings, and then we will utilize the pandas `get_dummies` method to create the binary features.  Binary features are often referred to in the statistics world as dummy features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tag_list'] = df['tags'].apply(lambda x: [elem['name'] for elem in eval(x)])\n",
    "tag_df = pd.get_dummies(df['tag_list'].apply(pd.Series).stack()).sum(level=0)\n",
    "# TODO - Explain how merges work or better yet figure a way to avoid merging. - Jack 11/10/17\n",
    "df = df.merge(tag_df, left_index = True, right_index = True, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[tag_df.columns] = df[tag_df.columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting features from strings\n",
    "\n",
    "String variables by themselves are generally not good inputs to algorithms; howevever, it is often possible to extract meaningful features from encoding the information that they contain.  Let's first find out which of our variables are string variables.  From there, let's review some of the variables and see if we can construct new features from the contents of these string variables.\n",
    "\n",
    "To discover which of our DataFrame columns are string variables, we will utilize the pandas `dtypes` method.  In pandas, there are the following types:\n",
    "\n",
    "\n",
    "\n",
    "|       dtype        |        Description        |\n",
    "|--------------------|---------------------------|\n",
    "|      float         | Numeric value with a decimal point.  If NaNs exist in col, pandas will default to float|\n",
    "|        int         | Numerica values without decimal points. |\n",
    "|       bool         | Column consisting of True and False|\n",
    "| datetime64[ns, tz] | Objects which contain specific date and time |\n",
    "|   timedelta[ns]    | Object which indicates time elapsed between two datetimes |\n",
    "|     category       | Variables that can only have specified values |\n",
    "|      object        | Pandas representation of string variables |\n",
    "\n",
    "Let's now use pandas method `get_dtype_counts` to see what data types exist in the DataFrame, and then select_dtypes to view all columns with dtype == object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The borrowers column looks like it may have some interesting information, but it is hard to tell since the string is cropped in the displayed DataFrame.  Lets take a look at an example value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['borrowers'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple feature we can create is count for the number of borrowers listed.  In order to accomplish this, we will leverage pandas [apply](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) method, which allows us to apply a lambda function to a specific column or collection of columns in order to create a new vector.  The provided lambda function is applied to each row in order to calculate the value of the corresponding row in the new vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_borrowers'] = df['borrowers'].apply(lambda x: x.count(\"{\"))\n",
    "df['num_tags'] = df['tags'].apply(lambda x: x.count(','))\n",
    "print(df[df['num_borrowers']>1]['num_borrowers'].iloc[0])\n",
    "print(df[df['num_borrowers']>1]['borrowers'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping in mind that the question that we are trying to answer is \"What drives the loan amount requested by KIVA borrowers?\" let's create a few variables that encode the information on the gender of the listed borrowers.\n",
    "\n",
    "In order to do this, we will once again use pandas' `apply` method, but this time will we introduce an if-else statement inside the lambda function.  This will enable us to change the value of the resulting column vector based on whether the conditional returns True or False for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['female']=df['borrowers'].apply(lambda x: 0 if x.split(\"gender': '\")[1][0]=='M' else 1)\n",
    "df['num_male'] = df['borrowers'].apply(lambda x: x.count('''M'''))\n",
    "df['num_female'] = df['borrowers'].apply(lambda x: x.count('''F'''))\n",
    "df['pct_female']=100.00*df['num_female']/(df['num_male']+df['num_female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, marital status and a boolean for whether or not they have kids.  These featuers will all be booleans, and in order to construct them we will use panda's [str.contains](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html).  This is a handy feature because it allows us to utilize a [regex](https://docs.python.org/2/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whether or not the borrower is widowed\n",
    "##  Note the str.contains function\n",
    "df['widowed'] = df['description.texts.en'].str.contains(\"widowed|widow\", na=False) * 1.0\n",
    "## Whether or not the borrower is married\n",
    "df['married'] = np.where(df['description.texts.en'].str.contains(\"married|husband|wife\", na=False)==True, 1, 0) * 1.0\n",
    "## Whether or not the borrower has children, notice we look for many variants of the word.\n",
    "df['kids'] = df['description.texts.en'].str.contains(\"kids|child|children|kid|son|daughter|mother|father|parents\", na=False) * 1.0\n",
    "df['parent'] = np.where(df['#Parent']==1, df['#Parent'], df['kids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age and number of children\n",
    "\n",
    "Creating variables for age will be a bit tricky. In the cells below, we parse out the age of the user by doing the following:\n",
    "\n",
    "1. Importing a CSV mapping of age strings that appear in the Kiva description field (e.g. \"2 years\") to the integer counterparts (e.g. 2).\n",
    "2. We then define a function to check for each of these string values within the description.texts.en field of our main dataframe. If a match is found, we append that string value to a new list, \"age\", which we create at the start of the function. If no match is found, we append a blank string. When the function has completed, we have a list the same length of our main dataframe, with the corresponding age string value for each observation (e.g. \"2 years\" or \" \" if there is no age value available)\n",
    "    1. We use functions from the regular expression package to perform the string searches within the description.texts.en. Specifically, we use re.compile and  re.findall functions to first compile all possible age strings of interest, and then find all instances of the corresponding string.\n",
    "3. We then create a new column in our main dataframe, \"age\", which is simply the list we created in step 2.\n",
    "4. Finally, we perform a left join of our main dataframe with the CSV mapping, to map the string versions of age with their integer counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_tags = pd.read_csv('../data/tags.csv')\n",
    "lookup_tags.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Age of borrower and number of children\n",
    "#  define a function that performs a loop that parses out all words, \n",
    "#  finds age and number of children match and creates a list that is return at the end of the function\n",
    "def text_search(tag):\n",
    "    #creates empty lists that are then added to in loop\n",
    "    number=[]\n",
    "    flag = tag.astype(str)\n",
    "    match=flag.tolist()\n",
    "    match = re.compile(r'\\b(?:%s)\\b' % '|'.join(match))\n",
    "    for descr in df['description.texts.en']:\n",
    "        try:\n",
    "            if isinstance(descr, str):\n",
    "                    if re.findall(match, descr):\n",
    "                        match_0=re.findall(match,  descr)\n",
    "                        match_1=re.findall(match,  descr)[:1]\n",
    "                        word_1=\" \".join(match_1)\n",
    "                        number.append(word_1)\n",
    "                    else:\n",
    "                        number.append('')\n",
    "        except:\n",
    "            print('error')\n",
    "            \n",
    "    return(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we write a small loop to go through each feature and search. Running this loop is fairly computationally expensive since it is doing a string match against every row of the data. You can expect it to take a few minutes to run. You can add other lists to the tags csv to extend the features you search for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['age','children_1', 'children_2'] \n",
    "for feature in features:\n",
    "    number= text_search(lookup_tags[feature])\n",
    "    df[feature]=pd.DataFrame(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we map the integer fields unto our dataframe. That way we can decide whether to use number of children as a str feature or an int feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = dict(zip(lookup_tags.children_1, lookup_tags.children_int))\n",
    "df['children_int_1'] = df['children_1'].map(mydict)\n",
    "\n",
    "mydict = dict(zip(lookup_tags.children_2, lookup_tags.children_int))\n",
    "df['children_int_2'] = df['children_2'].map(mydict)\n",
    "\n",
    "mydict = dict(zip(lookup_tags.age, lookup_tags.age_int))\n",
    "df['age_int'] = df['age'].map(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['children_int'] = df['children_int_1'].fillna(df['children_int_2'])\n",
    "df['children_int'] = df['children_int'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metadata\n",
    "\n",
    "We have data specifying which partners provided the loan for each row; however, this information alone is not that helpful.  Let's try to extract some metadata from the dataset to learn how impactful partners are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique partners: {0} \\n\".format(len(df['partner_name'].unique())))\n",
    "print(\"Top 15 partners: \\n{0}\\n\".format(df['partner_name'].value_counts().head(15)))\n",
    "print(\"Bottom 5 partners: \\n{0}\".format(df['partner_name'].value_counts().tail(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge disparity between the number of loans provided per partner.  This information could be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's only include those that have > 1000 obs (top 10)\n",
    "top_partners = df['partner_name'].value_counts().index[:10]\n",
    "top_partner_ids = df['partner_id'].value_counts().index[:10]\n",
    "df['top_partners'] = df['partner_name'].apply(lambda x: x if x in top_partners else \"Other\")\n",
    "df['top_partner_id'] = df['partner_id'].apply(lambda x: x if x in top_partner_ids else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=df['top_partners'], y=df['loan_amount'],showfliers=False)\n",
    "ax.set_xticklabels(labels=ax.get_xticklabels(), rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from Kiva that an exploratory partner who does not have a proven track record can be tested using a seed sum of $50,000. Let's create a boolean feature for exploratory partner in case we want to remove or otherwise treat these partners differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partner_dollar_amount = pd.DataFrame(df[(df['borrower_count'] == 1)].groupby(['partner_name','posted_year']).sum()['loan_amount'])\n",
    "partner_dollar_amount.columns = ['partner_dollar_amount']\n",
    "df = df.merge(partner_dollar_amount, left_on=['partner_name','posted_year'], right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['exploratory_partner']=np.where(df['partner_dollar_amount']>50000,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['borrower_count'] == 1)]['exploratory_partner'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling\n",
    "\n",
    "We will not overwrite our dataframe with scaled values because the appropriate scaling technique depends on the algorithm.  These are the three most common feature scaling techniques:\n",
    "1. Normalization\n",
    "2. Standardization\n",
    "3. Log-transform\n",
    "\n",
    "Normalization is the process of rescaling the data from 0-1.  The formula for this approach is:\n",
    "\n",
    "`X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.MinMaxScaler()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "normalized = min_max_scaler.fit_transform(df['loan_amount'].astype(np.float64).values.reshape(-1,1))[:,0]\n",
    "print(\"Pre Scaling\\tMin: {0}\\t\\t Max: {1}\\tMean: {2}\".format(df['loan_amount'].min(),df['loan_amount'].max(),df['loan_amount'].mean()))\n",
    "print(\"Post Scaling\\tMin: {0}\\t Max: {1}\\tMean: {2}\".format(np.min(normalized),np.max(normalized),np.mean(normalized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization assumes normally distributed data (i.e., Gaussian) and scales the data so that it has a zero mean and unit variance.  Below is the formula\n",
    "$${\\dfrac{x - \\bar x}{\\sigma}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized = preprocessing.scale(df['loan_amount'].astype(np.float64))\n",
    "print(\"Post Scaling\\tMin: {0}\\t Max: {1}\\tMean: {2}\".format(np.min(standardized),np.max(standardized),np.mean(standardized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these values, it appears that our data has a skewed distribution and is actually a good candidate for a log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['loan_amount'])\n",
    "plt.show()\n",
    "log_loan_amount = np.log(df['loan_amount'])\n",
    "plt.hist(log_loan_amount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Imputation / cleaning\n",
    "\n",
    "Missing data can be informative, but it also will prevent many algorithms from training.  In order to enable our models to train while preserving the fact that some data is missing, we are going to:\n",
    "1. Create a new column that indicates whether or not that column had missing data.\n",
    "In pandas, missing data is either represented as NaN (Not a Number), or NaT (Not a Time).  While we look at our missing data, let's look at strings, numeric, and time objects separateley.\n",
    "\n",
    "2. Impute missing data with the column's mean\n",
    "\n",
    "First, let's have a quick refresher on dyptes in our DataFrame and create lists of all of the columns for specific data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = df.select_dtypes(include=['datetime64','timedelta64']).columns\n",
    "str_columns = df.select_dtypes(include=[object]).columns\n",
    "numeric_columns = df.select_dtypes(exclude=[object,'datetime64','timedelta64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use pandas `isnull` and `sum` functions to see how many observations of each column are missing.\n",
    "Since there are a lot of columns in this DataFrame, let's restrict our returned DataFrame to columns which have \n",
    "missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[time_columns].isnull().sum()[df[time_columns].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[str_columns].isnull().sum()[df[str_columns].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_columns].isnull().sum()[df[numeric_columns].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With missing data, you should always check to see if there is a systemic difference between observations with and without missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['funded_date'].isnull()].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df['funded_date'].isnull()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create columns that indicate whether or not data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_columns:\n",
    "    df[col+'_na'] = pd.isnull(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing data with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE END!\n",
    "\n",
    "That is all for our feature engineering module!  Now that we have finished creating all of our features we can go ahead and explore them with some EDA!  The last step of this module is to save our results into a new csv.\n",
    "\n",
    "(We comment out this command because the data has already been saved in our data repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"../data/clean_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
