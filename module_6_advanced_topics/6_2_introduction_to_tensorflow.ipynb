{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow\n",
    "This notebook will introduce you to the basics of TensorFlow as well as walk through a linear regression example, a concept you should already be familiar with, with you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow sessions\n",
    "\n",
    "Every TensorFlow session takes care of placing the operations onto devices such as CPUs and GPUs and running them. There are two types of TensorFlow sessions:\n",
    "1. Regular sessions: require you to start and close the session, as well as declare the session as the live session in a separate \"block\"\n",
    "2. Interactive sessions: still requires you to start and close the session, but does not require you to declare the session as the live session in a separate block \n",
    "\n",
    "TensorFlow will automatically evaluate sets of nodes that depend on each other, e.g.:<br>\n",
    "\n",
    "w = tf.constant(3) <br>\n",
    "x = w + 2 <br>\n",
    "y = x + 5 <br>\n",
    "z = x + 3 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample regular session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# Set up constants and variables\n",
    "x = tf.Variable(3, name = \"x\")\n",
    "y = tf.Variable(4, name = \"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "# Evaluate the variables in a regular session\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "sess.close()\n",
    "\n",
    "# If you don't want to write sess.run() for every session, you can use a starting block\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample interactive session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# Set up constants and variables\n",
    "x = tf.Variable(3, name = \"x\")\n",
    "y = tf.Variable(4, name = \"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "# Evaluate the variables in an interactive session\n",
    "sess = tf.InteractiveSession()\n",
    "x.initializer.run()\n",
    "y.initializer.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also shorten your code by initializing all your variables at once using tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow graphs\n",
    "TensorFlow operations are conducted within the framework of a graph. You can create a TensorFlow graph very easily, as creating a node will add it to the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Add a node to the default graph\n",
    "x1 = tf.Variable(1)\n",
    "print(x1.graph is tf.get_default_graph())\n",
    "\n",
    "# Create a separate default graph, and add two nodes to it\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    x3 = tf.Variable(3)\n",
    "    \n",
    "# You can test which node the graph belongs to\n",
    "print(x2.graph is graph)\n",
    "\n",
    "# You can also reset your default graph when you've ran the same commands multiple times, \n",
    "# meaning you've added many duplicate nodes to the same graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we talk about linear regression, it is important to talk about some of the vocabulary in TensorFlow:\n",
    "* Ops: TensorFlow operations for multiplication and addition etc. take any number of inputs and produce any number of outputs\n",
    "* Tensors: Inputs and outputs are multidimensional arrays with a shape and type\n",
    "\n",
    "Deep learning relies on linear algebra (working with arrays) to make large calculations faster, instead of using for loops for instance. Also, TensorFlow doesn't make any calculations that are added to a graph until the graph is run. This also helps optimize run times.\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "#### Deterministic solution\n",
    "Linear regression has a deterministic solution using linear algebra. Let's use the deterministic solution to create a linear regression for a sample data set. Recall the linear regression least squares estimator solution for the coefficients is equal to:\n",
    "$$(X^TX)^{-1} \\cdot y$$\n",
    "<br>Before we write the deterministic solution for linear regression, we need to learn a few functions in TensorFlow:\n",
    "* matmul(A,B): matrix multiplication\n",
    "* matrix_inverse(A): inverse of a matrix\n",
    "* transpose(A): transpose of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "           37.88      , -122.23      ],\n",
       "        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "           37.86      , -122.22      ],\n",
       "        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "           37.85      , -122.24      ],\n",
       "        ...,\n",
       "        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "           39.43      , -121.22      ],\n",
       "        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "           39.43      , -121.32      ],\n",
       "        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "           39.37      , -121.24      ]]),\n",
       " 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'DESCR': 'California housing dataset.\\n\\nThe original database is available from StatLib\\n\\n    http://lib.stat.cmu.edu/datasets/\\n\\nThe data contains 20,640 observations on 9 variables.\\n\\nThis dataset contains the average house value as target variable\\nand the following input variables (features): average income,\\nhousing average age, average rooms, average bedrooms, population,\\naverage occupation, latitude, and longitude in that order.\\n\\nReferences\\n----------\\n\\nPace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\nStatistics and Probability Letters, 33 (1997) 291-297.\\n\\n'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's pull one of the datasets from sklearn and understand the format and data type for housing\n",
    "housing = fetch_california_housing()\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "California housing dataset.\n",
      "\n",
      "The original database is available from StatLib\n",
      "\n",
      "    http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The data contains 20,640 observations on 9 variables.\n",
      "\n",
      "This dataset contains the average house value as target variable\n",
      "and the following input variables (features): average income,\n",
      "housing average age, average rooms, average bedrooms, population,\n",
      "average occupation, latitude, and longitude in that order.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      "Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "Statistics and Probability Letters, 33 (1997) 291-297.\n",
      "\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# The data comes as a sklearn Bunch object\n",
    "print(type(housing))\n",
    "\n",
    "# A scklean Bunch is similar to a json-format for data or a Python dictionary. You can do things like call certain keys:\n",
    "print(housing['DESCR'])\n",
    "\n",
    "# The feature names are the column names for the data array, and the target variable is described in the 'DESCR' value\n",
    "print(len(housing['feature_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7185181e+01]\n",
      " [ 4.3633747e-01]\n",
      " [ 9.3952334e-03]\n",
      " [-1.0711310e-01]\n",
      " [ 6.4479220e-01]\n",
      " [-4.0338000e-06]\n",
      " [-3.7813708e-03]\n",
      " [-4.2348403e-01]\n",
      " [-4.3721911e-01]]\n",
      "9\n",
      "8\n",
      "(20640, 9) (20640, 8)\n"
     ]
    }
   ],
   "source": [
    "# We can use numpy arrays to work in TensorFlow. Recall the deterministic linear regression solution from above.\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name = \"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = \"y\")\n",
    "XT = tf.transpose(X)\n",
    "coefficients = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT),y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = coefficients.eval()\n",
    "    print(theta_value)\n",
    "    print(len(theta_value))\n",
    "sess.close()\n",
    "print(n)\n",
    "print(housing_data_plus_bias.shape, housing.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Checks for understanding:_\n",
    "* Why do we have to run tf.Session() as sess? Is it is a regular session or interactive session?\n",
    "* What happens when we run coefficients.eval()? Describe the order in which the variables are evaluated.\n",
    "* How do we interpret theta_value?\n",
    "* What is a limitation of running linear regression in this way?\n",
    "* Does the linear regression result contain a constant (y = aX + b) or not (y = aX)? Explain how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "\n",
    "We can also solve linear regression manually with gradient descent, whose calculations are optimized for speed when using TensorFlow. Before we begin linear regression with gradient descent, there are a few functions in TensorFlow to learn:\n",
    "* random_uniform(): creates a node in the default graph with random values in a matrix, given a shape and value range\n",
    "* assign(): assigns a new value to a variable, i.e. updating a variable versus declaring the value of a new variable node\n",
    "\n",
    "Recall the psuedo code describing what gradient descent does.\n",
    "\n",
    "$coefficents = random\\_uniform()\\\\\n",
    "for \\ n \\ iterations:\\\\\n",
    " \\  \\  \\ current\\_cost = mse(coefficients)\\\\\n",
    " \\  \\  \\ if \\ current\\_cost < tolerance:\\\\\n",
    " \\  \\  \\  \\  \\  \\ break\\\\\n",
    "\\  \\  \\ coefficients = coefficients - learning\\_rate*gradient\\\\\n",
    "return \\ coefficients$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 MSE =  7.469492\n",
      "Iteration: 100 MSE =  1.0072225\n",
      "Iteration: 200 MSE =  0.82173306\n",
      "Iteration: 300 MSE =  0.7433307\n",
      "Iteration: 400 MSE =  0.68697\n",
      "Iteration: 500 MSE =  0.64556336\n",
      "Iteration: 600 MSE =  0.61504304\n",
      "Iteration: 700 MSE =  0.59248376\n",
      "Iteration: 800 MSE =  0.57575727\n",
      "Iteration: 900 MSE =  0.56331325\n",
      "[[ 2.0685523 ]\n",
      " [ 0.87307703]\n",
      " [ 0.17952992]\n",
      " [-0.24919613]\n",
      " [ 0.2489019 ]\n",
      " [ 0.01704344]\n",
      " [-0.04539434]\n",
      " [-0.41602504]\n",
      " [-0.38697636]]\n"
     ]
    }
   ],
   "source": [
    "# Set some constants for the gradient descent algorithm\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Scale your data for gradient descent. We use the StandardScaler library\n",
    "m, n = housing.data.shape\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(housing.data)\n",
    "scaled_data = scaler.transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)), scaled_data]\n",
    "\n",
    "# Set your input data as data matrices\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name = \"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = \"y\")\n",
    "\n",
    "# Set the answer to a random matrix of coefficients using random_uniform\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n",
    "\n",
    "# Calculate the current_cost function value\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "\n",
    "# Calculate the gradient (a multi-variable derivative) and the new theta value\n",
    "# See https://spin.atomicobject.com/wp-content/uploads/linear_regression_gradient1.png to understand the math behind the gradient\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate*gradients)\n",
    "\n",
    "# This is a shortcut in TensorFlow to initialize all constants/variables at once\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Evaluate the session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Iteration:\", epoch, \"MSE = \", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Checks for understanding?_\n",
    "* Why is it important to scale your data when using gradient descent? What will happen if you do not do so?\n",
    "* How do we interpret the coefficients in this instance, and why are they different from before?\n",
    "* Why is theta.eval() written after the for loop for calculating training_op?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, in the above example we are calculating gradients with a formula using what we know about the partial derivative as well as finding the minimum MSE using for loops. However, TensorFlow has a built in gradient function as well as a built in optimizer framework to get around these manual calculations.\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)<br>\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and visualizing models\n",
    "TensorFlow provides us an easy way to save and visualize models. We will save the gradient descent linear regression model (updated with the built-in functions mentioned) as an example below. We just add two lines:\n",
    "\n",
    "saver = tf.train.Saver() <br>\n",
    "save_path = saver.save(sess, '/tmp/gradient_descent_sample.ckpt')<br>\n",
    "saver.restore(sess, 'temp/gradient_descent_sample.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.6959382 ]\n",
      " [ 0.36700886]\n",
      " [-0.53080046]\n",
      " [ 0.17899923]\n",
      " [-0.21365108]\n",
      " [ 0.87984574]\n",
      " [-0.75590307]\n",
      " [-0.57794285]\n",
      " [-0.02671256]]\n"
     ]
    }
   ],
   "source": [
    "# Set some constants for the gradient descent algorithm\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Scale your data for gradient descent. We use the StandardScaler library\n",
    "m, n = housing.data.shape\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(housing.data)\n",
    "scaled_data = scaler.transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)), scaled_data]\n",
    "\n",
    "# Set your input data as data matrices\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name = \"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = \"y\")\n",
    "\n",
    "# Set the answer to a random matrix of coefficients using random_uniform\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n",
    "\n",
    "# Calculate the current_cost function value\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "\n",
    "# Calculate the gradient (a multi-variable derivative) and the new theta value\n",
    "# See https://spin.atomicobject.com/wp-content/uploads/linear_regression_gradient1.png to understand the math behind the gradient\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# This is a shortcut in TensorFlow to initialize all constants/variables at once\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Initialize the saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Evaluate the session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)\n",
    "    save_path = saver.save(sess, '/tmp/gradient_descent_sample.ckpt') # Add a save path for your model\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/gradient_descent_sample.ckpt\n",
      "[[-0.6959382 ]\n",
      " [ 0.36700886]\n",
      " [-0.53080046]\n",
      " [ 0.17899923]\n",
      " [-0.21365108]\n",
      " [ 0.87984574]\n",
      " [-0.75590307]\n",
      " [-0.57794285]\n",
      " [-0.02671256]]\n"
     ]
    }
   ],
   "source": [
    "# To pull up the saved model, you can use the restore function to, for instance, print again the best theta,\n",
    "# which we had stored in the previous session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '/tmp/gradient_descent_sample.ckpt')\n",
    "    print(best_theta)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can visualize our models in TensorFlow using Google's TensorBoard web server and writing a summary of your model to a log directory. This will involve setting up a log file location and writing to a summary file. After saving your data, you can open a tensorboard session and go to localhost:6006. Note that 6006 is \"goog\" upside down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5264683 ]\n",
      " [ 0.46899325]\n",
      " [-0.90169144]\n",
      " [ 0.7137366 ]\n",
      " [ 0.04586203]\n",
      " [ 0.34158915]\n",
      " [-0.01591481]\n",
      " [-0.8304522 ]\n",
      " [-0.7211892 ]]\n"
     ]
    }
   ],
   "source": [
    "# Recreate model while writing to a logdir\n",
    "\n",
    "# Set up the logdir location\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "# Set some constants for the gradient descent algorithm\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Scale your data for gradient descent. We use the StandardScaler library\n",
    "m, n = housing.data.shape\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(housing.data)\n",
    "scaled_data = scaler.transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m,1)), scaled_data]\n",
    "\n",
    "# Set your input data as data matrices\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name = \"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype = tf.float32, name = \"y\")\n",
    "\n",
    "# Set the answer to a random matrix of coefficients using random_uniform\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name = \"theta\")\n",
    "\n",
    "# Calculate the current_cost function value\n",
    "y_pred = tf.matmul(X, theta, name = \"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")\n",
    "\n",
    "# Calculate the gradient (a multi-variable derivative) and the new theta value\n",
    "# See https://spin.atomicobject.com/wp-content/uploads/linear_regression_gradient1.png to understand the math behind the gradient\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# This is a shortcut in TensorFlow to initialize all constants/variables at once\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Initialize the saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Evaluate the session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    print(best_theta)\n",
    "    mse = tf.summary.scalar('MSE', mse)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open tensorboard with this text. You can also run this code in your command prompt\n",
    "!tensorboard --logdir tf_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: O'Reilly Hands-On Machine Learning with Scikit-Learn & TensorFlow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
