{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2: Random Forests\n",
    "\n",
    "Following on from Lab 4.1: Decision trees, the Random Forest algorithm is based on Decisions Trees and can be used for either regression or classification tasks. To overcome Decisions Trees tendancy to overfit, Random Forests aggregate the results from many decision trees with an [ensemble approach](http://scikit-learn.org/stable/modules/ensemble.html). \n",
    "\n",
    "In this lesson we focus on Random Forests, which use bagging to prevent overfitting by only allowing a given decision tree to optimize for a random subset of the data for every tree, and a random subset of the input features at every split. (Don't worry if this does not make too much sense right now, we'll work through together  ðŸ˜€). \n",
    "\n",
    "Here's a look ahead at what we'll be doing in this notebook:\n",
    "\n",
    "1. Import packages\n",
    "2. Load data\n",
    "3. Bagging\n",
    "4. Build a Random Forest\n",
    "5. Tune parameters\n",
    "6. Feature importance\n",
    "\n",
    "A quick refresher on Bagging:\n",
    "- Grow multiple decisions trees from random subsets of the data\n",
    "- All features are considered at every split in the decision tree\n",
    "- The result is the average prediction of all trees\n",
    "\n",
    "A quick refresher on the Random Forest theory:\n",
    "- Random Forest is based on Decision Trees -> many trees = a forest!\n",
    "- Grows multiple trees on random subsets of the parent dataset\n",
    "- **At every split, a new random subset of features is chosen**\n",
    "- This leads to \"decorrelated\" trees which leads to a large increase in performance!\n",
    "\n",
    "Again, we will be looking at how we can predict the loan amount using the random forest algorithm. \n",
    "\n",
    "Here is a very [simple example](https://bicorner.com/2015/10/26/random-forest-using-ipython/) of how the RandomForestClassifier works: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages\n",
    "<a id='loaddata'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and format data\n",
    "<a id='loaddata'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to pandas DataFrame\n",
    "import pandas as pd\n",
    "data_path = '../data/'\n",
    "df = pd.read_csv(data_path+'clean_data.csv.zip', \n",
    "                 low_memory=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose a limited subset of data to conduct the analysis for the sake of training time. In practice, we should use more features. This is a mix of numberic and one hot-coded catergorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df[['loan_amount', \n",
    "           'partner_delinquency_rate',\n",
    "            'posted_year',\n",
    "           'posted_month',\n",
    "           'num_tags',\n",
    "           '#Woman Owned Biz',\n",
    "           'age_int',\n",
    "           '#Repeat Borrower',\n",
    "           'children_int',\n",
    "          'terms.repayment_term',\n",
    "           'pct_female',\n",
    "           'exploratory_partner',\n",
    "           'partner_dollar_amount',\n",
    "           'top_partner_id',\n",
    "           'days_to_fund']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cols['loan_amount']\n",
    "# drop returns a copy of the DataFrame with the specified columns removed.  \n",
    "X = cols.drop('loan_amount', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets;\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bagging\n",
    "<a id='bagging'></a>\n",
    "\n",
    "As Decision Trees naturally overfit to the data, better performance is achieved when an **ensemble approach** such as Bagging is used.\n",
    "\n",
    "Bagging is a shorter term for Bootstrapping Aggregation which is a general procedure to reduce variance in models that have a tendancy to overfit.\n",
    "\n",
    "This diagram below gives a visual overview of how bagging works in the context of a classifying algorithm.\n",
    "\n",
    "<img src=\"./images/bagging.png\" alt=\"bagging\" style=\"width: 500px;height=500\"/>\n",
    "\n",
    "As an ensemble method, bagging does the following:\n",
    "- Creates N random subsamples of the dataset using selection with replacement. (Statistically 1/3 of the data is left of each subsample)\n",
    "- Trains a new decision tree on each N subsample\n",
    "- Takes the prediction from each and every N tree and averages the result\n",
    "- This average is the final output of the bagging model. \n",
    "\n",
    "This **Ensemble approach** uses the results of several random analyses to find an average response.\n",
    "\n",
    "For each tree, there will be a sample of data which was not used to train the algorithm. This means that we can also measure the **out-of-bag** score ( or **oob_score**). This score evaluates the performance of the model on the sample not used while training an individual tree. (On average, a random ~1/3 of data are not used to train a given tree).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Random Forest\n",
    "<a id='randomforest'></a>\n",
    "\n",
    "Even with Bagging, the decision trees can be largely correlated. This is because given subsets from the same dataset, the trees are probably going to split on the same features and result in very similar predictions.\n",
    "\n",
    "To de-correlate the trees, we use a Random Forest Regressor, which at each split, only consider a random subsample of the features! This means that the trees are forced to make different split decisions and results in less correlated trees.\n",
    "\n",
    "**A Random Forest is essentially bagged decision trees where each decision tree only considers a random subset of the features at each split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many columns our dataset has...\n",
    "print(f\"Number of features =  {X_train.shape[1]}\")\n",
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the standard sklearn steps: \n",
    "- **Step 1: Initiate the algorithm** Define the parameters (& hyperparameters of the algorithm) of the algorithm. For example, the maximum depth, the minimum samples in a leaf etc. (Check documentation for more information)\n",
    "\n",
    "- **Step 2: Train the algorithm** Train the algorithm by fitting it to the X_train and y_train datasets.\n",
    "\n",
    "- **Step 3: Evaluating the algorithm** Evaluate the predictive power of the algorithm by comparing the predictive loan amount values to the true values. We can do this for the training and testing dataset.\n",
    "\n",
    "We'll use the same function we created in the previous lab to combine these into a single step with the additional option of printing the oob_score, which we should use when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_score_regressor(sklearn_regressor, X_train, y_train, X_test, y_test, model_parameters,print_oob_score=False):\n",
    "    '''\n",
    "    Purpose: \n",
    "        - train a regressor on training data\n",
    "        - score data on training and test data\n",
    "        - return trained model\n",
    "    '''\n",
    "    # Step 1: Initializing the sklearn regressor \n",
    "    regressor = sklearn_regressor(**model_parameters)\n",
    "    # Step 2: Training the algorithm using the X_train dataset of features and y_train, the associated target features\n",
    "    regressor.fit(X_train, y_train)\n",
    "    # Step 3: Calculating the score of the predictive power on the training and testing dataset.\n",
    "    training_score = regressor.score(X_train, y_train)\n",
    "    testing_score = regressor.score(X_test, y_test)\n",
    "    # Print the results\n",
    "    print(f\"Train score: {training_score:.4}\")\n",
    "    print(f\"Test score: {testing_score:.4}\")\n",
    "    if print_oob_score:\n",
    "        print(f\"OOB score: {regressor.oob_score_:.4}\")\n",
    "        \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Random Forest algorithm instead of a single Decision tree introduces some new paramters that can be used to tune the model. Three new and important parameters are:\n",
    "- n_estimaters = During the bagging, the number of subsamples to create and therefore the number of individual decision trees to train. \n",
    "- max_features = The maximum number of random features to consider at each split\n",
    "- oob_score = whether or not to calculate the oob_score (described in bagging above)\n",
    "\n",
    "To start we will randomly assign a value to these parameters and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters\n",
    "parameters = {\"oob_score\":True, \"n_estimators\": 50, \n",
    "              \"max_features\": 10, 'random_state':42}\n",
    "\n",
    "trained_regressor = train_score_regressor(sklearn_regressor=RandomForestRegressor,\n",
    "                                          X_train=X_train, y_train=y_train, \n",
    "                                          X_test=X_test, y_test=y_test, \n",
    "                                          model_parameters=parameters,\n",
    "                                          print_oob_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R^2 of the OOB score above tell us the model is performing well on unseen data. This is confirmed by the comparable R^2 for the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a graph of the true values vs the predicted values for the training and test datasets\n",
    "def plot_y_yhat_scatter(y_actual,y_predicted,train_test):\n",
    "    ax = sns.regplot(x=y_actual, y=y_predicted, fit_reg=False)\n",
    "    ax.set_xlabel('true values')\n",
    "    ax.set_ylabel('predicted values')\n",
    "    ax.set_title('Relationship between true and predicted loan amounts: '+train_test+' results')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_yhat_scatter(y_train, trained_regressor.predict(X_train),train_test=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_yhat_scatter(y_test, trained_regressor.predict(X_test),train_test=\"testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tune Parameters\n",
    "<a id='parameters'></a>\n",
    "\n",
    "Again, how do we know the best combination of hyperparameters? \n",
    "\n",
    "**Cross validated Grid Search!**\n",
    "\n",
    "We're searching over a small range here to save time but in practice, we should search over a bigger range of hyperparameters. \n",
    "\n",
    "This can take a while "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "parameters = {'max_depth':[7,10,15], \n",
    "              'max_features': ['sqrt', 5, 10],\n",
    "              'n_estimators': [50,100,200]}\n",
    "# initialize model\n",
    "rf_regressor= RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "# initialize grid search and fit\n",
    "regressor = GridSearchCV(rf_regressor, parameters)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out what GridSearchCV found to be the best parameters \n",
    "regressor.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best parameters from grid search\n",
    "parameters = regressor.best_estimator_.get_params()\n",
    "# set the oob_score to True. This defaulted to False in the gridsearchCV to save time\n",
    "parameters['oob_score'] = True\n",
    "# train and evaluate our model\n",
    "trained_regressor = train_score_regressor(sklearn_regressor=RandomForestRegressor,\n",
    "                                          X_train=X_train, y_train=y_train, \n",
    "                                          X_test=X_test, y_test=y_test, \n",
    "                                          model_parameters=parameters,\n",
    "                                          print_oob_score=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best R^2 achieved with the Decision Tree algorithm alone in Lab 4.1 gave us a train and test score of 0.72 and 0.64 respectively, whereas with the Random Forest Algorithm, they have increased to 0.83 and 0.72 respectively. \n",
    "\n",
    "**We have see that there has been a vast improvement using the Random Forest over the Decision Tree algorithm!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_yhat_scatter(y_train, trained_regressor.predict(X_train),train_test=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_yhat_scatter(y_test, trained_regressor.predict(X_test),train_test=\"testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance\n",
    "<a id='featureimportance'></a>\n",
    "Remember the magnitude of the 'importance' is not indicative of how important the feature is, only the order matters!\n",
    "\n",
    "For example,\n",
    "- feature A has an importance of 0.5 \n",
    "- feature B has an importance of 0.25. \n",
    "\n",
    "Then all we can take away is that feature A is more explains more variance then feautre B, **not** that feature A explains twice as much as feature B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances from our final trained model...\n",
    "importances = trained_regressor.feature_importances_\n",
    "\n",
    "# Find the indices of the feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plotting a bar chart of feature importances in descending order\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.barplot(y=X_train.columns[indices],x=importances[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there is not a very clear relationship between any single feature and the loan_amount. The most important feature borrower count for One Acre Fund during their high loan period - this is very specific to just a small subset of the data. \n",
    "\n",
    "However the aggregate of these features together into the decision leads to effective predictions (R^2 ~ 0.63). This is a testament to the predictive power of Random Forests!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
