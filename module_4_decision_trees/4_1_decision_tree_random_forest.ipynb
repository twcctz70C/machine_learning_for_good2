{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree, Bagging, and Random Forests\n",
    "=====\n",
    "\n",
    "Decision trees, bagging, and random forecasts can be used for either regression or classification tasks. Decision trees are a powerful tool; however, are very prone to overfitting the training dataset and therefore often fail to generalize well to test data sets.  To overcome Decisions Trees tendency to overfit, one can aggregate the results from many decision trees with an [ensemble approach](http://scikit-learn.org/stable/modules/ensemble.html). In this lesson we will focus on Random Forests, which prevent overfitting by only allowing a given decision tree to optimize for a random subset of the data for every tree, and a random subset of the input features at every split (Don't worry if this does not make too much sense right now, we'll work through togetherÂ  ðŸ˜€). \n",
    "\n",
    "\n",
    "<img src=\"../images/DecisionTreeExample.png\" alt=\"Drawing\" style=\"width: 500px;height=500\"/>\n",
    "\n",
    "A quick refresher on Bagging:\n",
    "- Grow multiple decisions trees from random subsets of the data\n",
    "- All features are considered at every split in the decision tree\n",
    "- The result is the average prediction of all trees\n",
    "\n",
    "A quick refresher on the Random Forest theory:\n",
    "- Random Forest is based on Decision Trees -> many trees = a forest!\n",
    "- Grows multiple trees on random subsets of the parent dataset\n",
    "- **At every split, a new random subset of features is chosen**\n",
    "- This leads to \"decorrelated\" trees which leads to a large increase in performance!\n",
    "\n",
    "Here's a look ahead at what we'll be doing in this notebook:\n",
    "\n",
    "\n",
    "1. [Load data and packages](#loaddata)   \n",
    "  1.1 Load python packages  \n",
    "  1.2 Set Jupyter Notebook preferences  \n",
    "  1.3 Load Data\n",
    "    \n",
    "2. [Build Decision Tree](#decisiontree)\n",
    "\n",
    "3. [Build Bagged Decision Tree](#bagging)\n",
    "\n",
    "4. [Build Random Forest](#randomforest)\n",
    "\n",
    "5. [Feature Importance](#featureimportance)\n",
    "\n",
    "We'll work incrementally from Decision Trees to Random Forests. At each stage we will experiment with tuning the model parameters and evaluate the models performance.\n",
    "\n",
    "----\n",
    "\n",
    "Our previous Linear Regression model with assumes linearity among others. Whereas decision trees and associated algorithms are models that no longer restricted to independent variables which have a linear relationship and we do not have to ensure several assumptions are true. \n",
    "\n",
    "Therefore we can start to bring in other features that could be useful.\n",
    "\n",
    "After we run our decision trees, we will compare our new output to our output from the linear regressions we ran in the previous notebook. \n",
    "\n",
    "In this notebook, we will be looking at how we can predict the loan amount using decision trees, bagged decisions trees and the random forest algorithm. \n",
    "\n",
    "Here is visual introduction to [decision trees](https://algobeans.com/2016/07/27/decision-trees-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages\n",
    "<a id='loaddata'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphviz \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have graphviz installed or are having problems displaying the tree structure later on, try:\n",
    "\n",
    "Mac/Windows:\n",
    "\n",
    "```bash\n",
    "$ brew install graphviz \n",
    "```\n",
    "\n",
    "Linux:\n",
    "\n",
    "```\n",
    "$ sudo apt-get install graphviz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data to pandas DataFrame\n",
    "data_path = '../data/'\n",
    "df = pd.read_csv(data_path+'df_end_of_linear.csv', \n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Remove this part and incorporate it into feature engineering section.\n",
    "# potentially the feature engineering section should show how to do one hot encoding and this module\n",
    "# will reference one hot encoding and remove any cols that are strings to highligh that sklearn's implementation\n",
    "# cannot handle strings.\n",
    "df = df.dropna()\n",
    "cols = df[['loan_amount', \n",
    "           'partner_delinquency_rate',\n",
    "            'posted_year',\n",
    "           'posted_month',\n",
    "           'female',\n",
    "           'num_tags',\n",
    "           'parent',\n",
    "           'tag_#Woman Owned Biz',\n",
    "           'age_int',\n",
    "           'tag_#Repeat Borrower',\n",
    "           'children_int',\n",
    "          'more_one_partner_country',\n",
    "          'terms.repayment_term',\n",
    "           'tag_#Schooling',\n",
    "           'married',\n",
    "           'pct_female',\n",
    "           'exploratory_partner',\n",
    "           'partner_dollar_amount',\n",
    "           'top_partner_id',\n",
    "           'num_partner_countries',\n",
    "           'days_to_fund',\n",
    "            'hours_to_fund',\n",
    "            'bc_partner_others',\n",
    "           'bc_partner_HIHEA',\n",
    "           'bc_partner_OAF_high',\n",
    "           'bc_partner_OAF_low',\n",
    "         'sector_Personal Use',\n",
    "           'sector_Health',\n",
    "           'sector_Wholesale',\n",
    "           'sector_Agriculture',\n",
    "           'kids']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like last module, we are going to build regressors to predict the loan amount.\n",
    "\n",
    "However, instead of using just a few features, we are going to initially build a tree that considers many the features in the dataset - including those we have engineered ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['loan_amount']\n",
    "# drop returns a copy of the DataFrame with the specified columns removed.  \n",
    "X = cols.drop('loan_amount', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree\n",
    "<a id='decisiontree'></a>\n",
    "Before we build our first decision tree, let's first learn about the input parameters for sklearn's implementation of a Decision Tree Regressor.  Feel free to look at the [docs](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor.get_params), or simply put a question mark before a call to the class.  Prepending a ? to any method, variable, or class will display that method's defined docstring (way to go ipython!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DecisionTreeRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the sklearn algorithms are implemented using the same standard steps: \n",
    "- **Step 1: Initiate the algorithm** This is where we define the parameters (& hyperparameters of the algorithm) of the algorithm. For example, this is where we would define the maximum depth, the minimum samples in a leaf etc. To learn more about the parameters for each algorithm, either check the module documentation on the internet or run a cell with the algorithm name followed by ? as we did at the beginning of this notebook for the RandomForestRegressor. These resources will also tell you the default values used for each parameter. \n",
    "\n",
    "- **Step 2: Train the algorithm** This is where we train the algorithm by fitting it to the X_train and y_train datasets.\n",
    "\n",
    "- **Step 3: Evaluating the algorithm** This is where we evaluate the predictive power of the algorithm by comparing the predictive loan amount values to the true values. We can do this for the training and testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let build a function which encapsulates the 3 model implementation steps; Initialize, Train, Evaluate for any given sklearn regressor. \n",
    "\n",
    "We've included an option to calculate the 'Out Of Bag Score' - remember this from the lectures? This is an average of the model performance on Out Of Bag data during bagging. If this doesn't sound familiar, check out this blog to refresh your memory on [bagging and ensemble methods](https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/). \n",
    "\n",
    "We will also discuss this further below in the [bagging](#bagging) section of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_score_regressor(sklearn_regressor, X_train, y_train, X_test, y_test, model_parameters,print_oob_score=False):\n",
    "    '''\n",
    "    Purpose: \n",
    "        - train a regressor on training data\n",
    "        - score data on training and test data\n",
    "        - return trained model\n",
    "    '''\n",
    "    # Step 1: Initializing the sklearn regressor \n",
    "    regressor = sklearn_regressor(**model_parameters)\n",
    "\n",
    "    # Step 2: Training the algorithm using the X_train dataset of features and y_train, the associated target features\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Calculating the score of the predictive power on the training and testing dataset.\n",
    "    training_score = regressor.score(X_train, y_train)\n",
    "    testing_score = regressor.score(X_test, y_test)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Train score: {training_score:.4}\")\n",
    "    print(f\"Test score: {testing_score:.4}\")\n",
    "    if print_oob_score:\n",
    "        print(f\"OOB score: {regressor.oob_score_:.4}\")\n",
    "        \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all tree algorithms the major challenge is using the parameters to balance the bias vs variance tradeoff.  Before we get into exploring the parameters, let's see how the model preforms while using the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.0\n",
      "Test score: 0.3636\n"
     ]
    }
   ],
   "source": [
    "trained_regressor = train_score_regressor(sklearn_regressor = DecisionTreeRegressor,\n",
    "                                          X_train = X_train, y_train = y_train, \n",
    "                                          X_test = X_test, y_test = y_test, \n",
    "                                          model_parameters = {'random_state':42})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Decision Tree managed to get a perfect r2 scored on the training data, it managed an abysmal .34 on the test data.  This is a clear indication that the model has overfit the data.  Given that by default sklearn's implementation of a DecisionTreeRegressor does not put any restrictions on the depth of the tree, the number of samples per leaf, the number of samples per leaf, etc.  As a result, the Decision Tree will find signal in any and all noise of the training data set, which causes the model to perform poorly on the test data.  When a model overfits to a training data set, we say it has **high variance**.  Since an unconstrained decision tree will almost perfectly model any training data, it will vary tremendously depending on the training data that is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning\n",
    "To reduce the variance, let's now try to constrain the model using some of the provided parameters. Some of the main parameters that can be tuned are:\n",
    "- Criterion\n",
    "- Maximum depth of the tree\n",
    "- Minimum samples for each node split\n",
    "- Minimum samples for each terminal node\n",
    "- Maximum number of terminal nodes\n",
    "\n",
    "If you need a refresher to remember what these parameters are, look back over the class notes or use this [useful blog](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/#four).\n",
    "\n",
    "Initially, we are going to experiment with the max_depth parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.558\n",
      "Test score: 0.4383\n"
     ]
    }
   ],
   "source": [
    "trained_regressor = train_score_regressor(sklearn_regressor=DecisionTreeRegressor,\n",
    "                                          X_train=X_train, y_train=y_train, \n",
    "                                          X_test=X_test, y_test=y_test, \n",
    "                                          model_parameters = {\"max_depth\":4,\n",
    "                                                              'random_state':42})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the training r2 score plummeted down to .54, the test r2 score increased to .44, and since the goal is develop a model that accurately predict data we have never seen, that is the data we care about!\n",
    "\n",
    "Now that we have increased preformance, let's take a look at what the Decision Tree looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"2212pt\" height=\"458pt\"\n",
       " viewBox=\"0.00 0.00 2211.80 458.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 454)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-454 2207.7959,-454 2207.7959,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.031373\" stroke=\"#000000\" d=\"M1182.4671,-450C1182.4671,-450 1018.115,-450 1018.115,-450 1012.115,-450 1006.115,-444 1006.115,-438 1006.115,-438 1006.115,-398 1006.115,-398 1006.115,-392 1012.115,-386 1018.115,-386 1018.115,-386 1182.4671,-386 1182.4671,-386 1188.4671,-386 1194.4671,-392 1194.4671,-398 1194.4671,-398 1194.4671,-438 1194.4671,-438 1194.4671,-444 1188.4671,-450 1182.4671,-450\"/>\n",
       "<text text-anchor=\"start\" x=\"1013.9531\" y=\"-434.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">bc_partner_OAF_high â‰¤ 2.5</text>\n",
       "<text text-anchor=\"start\" x=\"1042.1069\" y=\"-420.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 109470.395</text>\n",
       "<text text-anchor=\"start\" x=\"1047.2988\" y=\"-406.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 11754</text>\n",
       "<text text-anchor=\"start\" x=\"1050.2759\" y=\"-392.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 409.748</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.027451\" stroke=\"#000000\" d=\"M878.7842,-350C878.7842,-350 767.7979,-350 767.7979,-350 761.7979,-350 755.7979,-344 755.7979,-338 755.7979,-338 755.7979,-298 755.7979,-298 755.7979,-292 761.7979,-286 767.7979,-286 767.7979,-286 878.7842,-286 878.7842,-286 884.7842,-286 890.7842,-292 890.7842,-298 890.7842,-298 890.7842,-338 890.7842,-338 890.7842,-344 884.7842,-350 878.7842,-350\"/>\n",
       "<text text-anchor=\"start\" x=\"763.7944\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">days_to_fund â‰¤ 4.5</text>\n",
       "<text text-anchor=\"start\" x=\"769.5161\" y=\"-320.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 69535.311</text>\n",
       "<text text-anchor=\"start\" x=\"770.2988\" y=\"-306.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 11718</text>\n",
       "<text text-anchor=\"start\" x=\"773.2759\" y=\"-292.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 399.038</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1011.5035,-385.9467C975.776,-373.0487 934.9598,-358.3137 900.49,-345.8697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"901.5399,-342.5276 890.9455,-342.424 899.1629,-349.1117 901.5399,-342.5276\"/>\n",
       "<text text-anchor=\"middle\" x=\"901.5702\" y=\"-360.854\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.760784\" stroke=\"#000000\" d=\"M1456.4671,-350C1456.4671,-350 1292.115,-350 1292.115,-350 1286.115,-350 1280.115,-344 1280.115,-338 1280.115,-338 1280.115,-298 1280.115,-298 1280.115,-292 1286.115,-286 1292.115,-286 1292.115,-286 1456.4671,-286 1456.4671,-286 1462.4671,-286 1468.4671,-292 1468.4671,-298 1468.4671,-298 1468.4671,-338 1468.4671,-338 1468.4671,-344 1462.4671,-350 1456.4671,-350\"/>\n",
       "<text text-anchor=\"start\" x=\"1287.9531\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">bc_partner_OAF_high â‰¤ 5.5</text>\n",
       "<text text-anchor=\"start\" x=\"1316.623\" y=\"-320.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 918211.806</text>\n",
       "<text text-anchor=\"start\" x=\"1332.4619\" y=\"-306.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 36</text>\n",
       "<text text-anchor=\"start\" x=\"1320.3828\" y=\"-292.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 3895.833</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>0&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1188.1169,-385.9467C1216.3842,-375.6302 1247.8719,-364.1384 1276.7916,-353.5837\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1278.2984,-356.7597 1286.4924,-350.0433 1275.8985,-350.1839 1278.2984,-356.7597\"/>\n",
       "<text text-anchor=\"middle\" x=\"1275.9468\" y=\"-368.5102\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.011765\" stroke=\"#000000\" d=\"M535.8228,-250C535.8228,-250 360.7592,-250 360.7592,-250 354.7592,-250 348.7592,-244 348.7592,-238 348.7592,-238 348.7592,-198 348.7592,-198 348.7592,-192 354.7592,-186 360.7592,-186 360.7592,-186 535.8228,-186 535.8228,-186 541.8228,-186 547.8228,-192 547.8228,-198 547.8228,-198 547.8228,-238 547.8228,-238 547.8228,-244 541.8228,-250 535.8228,-250\"/>\n",
       "<text text-anchor=\"start\" x=\"356.5254\" y=\"-234.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">terms.repayment_term â‰¤ 13.5</text>\n",
       "<text text-anchor=\"start\" x=\"394\" y=\"-220.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 53173.364</text>\n",
       "<text text-anchor=\"start\" x=\"398.6758\" y=\"-206.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 6534</text>\n",
       "<text text-anchor=\"start\" x=\"398.2759\" y=\"-192.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 325.895</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M755.3813,-299.8908C700.3736,-285.222 621.7973,-264.2684 557.8993,-247.2289\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"558.7442,-243.8319 548.18,-244.6371 556.9405,-250.5956 558.7442,-243.8319\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.047059\" stroke=\"#000000\" d=\"M910.8228,-250C910.8228,-250 735.7592,-250 735.7592,-250 729.7592,-250 723.7592,-244 723.7592,-238 723.7592,-238 723.7592,-198 723.7592,-198 723.7592,-192 729.7592,-186 735.7592,-186 735.7592,-186 910.8228,-186 910.8228,-186 916.8228,-186 922.8228,-192 922.8228,-198 922.8228,-198 922.8228,-238 922.8228,-238 922.8228,-244 916.8228,-250 910.8228,-250\"/>\n",
       "<text text-anchor=\"start\" x=\"731.5254\" y=\"-234.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">terms.repayment_term â‰¤ 13.5</text>\n",
       "<text text-anchor=\"start\" x=\"769\" y=\"-220.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 74916.177</text>\n",
       "<text text-anchor=\"start\" x=\"773.6758\" y=\"-206.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 5184</text>\n",
       "<text text-anchor=\"start\" x=\"773.2759\" y=\"-192.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 491.228</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>1&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M823.291,-285.8089C823.291,-277.6906 823.291,-268.8517 823.291,-260.3186\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"826.7911,-260.1307 823.291,-250.1308 819.7911,-260.1308 826.7911,-260.1307\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"transparent\" stroke=\"#000000\" d=\"M281.6875,-150C281.6875,-150 110.8945,-150 110.8945,-150 104.8945,-150 98.8945,-144 98.8945,-138 98.8945,-138 98.8945,-98 98.8945,-98 98.8945,-92 104.8945,-86 110.8945,-86 110.8945,-86 281.6875,-86 281.6875,-86 287.6875,-86 293.6875,-92 293.6875,-98 293.6875,-98 293.6875,-138 293.6875,-138 293.6875,-144 287.6875,-150 281.6875,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"106.8428\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">num_partner_countries â‰¤ 3.0</text>\n",
       "<text text-anchor=\"start\" x=\"145.8931\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 34171.97</text>\n",
       "<text text-anchor=\"start\" x=\"146.6758\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3435</text>\n",
       "<text text-anchor=\"start\" x=\"146.2759\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 269.185</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M367.5168,-185.9467C341.7452,-175.7199 313.0632,-164.3381 286.6564,-153.8593\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"287.626,-150.4786 277.0401,-150.0433 285.0441,-156.985 287.626,-150.4786\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.027451\" stroke=\"#000000\" d=\"M555.3013,-150C555.3013,-150 341.2807,-150 341.2807,-150 335.2807,-150 329.2807,-144 329.2807,-138 329.2807,-138 329.2807,-98 329.2807,-98 329.2807,-92 335.2807,-86 341.2807,-86 341.2807,-86 555.3013,-86 555.3013,-86 561.3013,-86 567.3013,-92 567.3013,-98 567.3013,-98 567.3013,-138 567.3013,-138 567.3013,-144 561.3013,-150 555.3013,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"337.0361\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">partner_dollar_amount â‰¤ 2124200.0</text>\n",
       "<text text-anchor=\"start\" x=\"394\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 66718.891</text>\n",
       "<text text-anchor=\"start\" x=\"398.6758\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3099</text>\n",
       "<text text-anchor=\"start\" x=\"398.2759\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 388.754</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M448.291,-185.8089C448.291,-177.6906 448.291,-168.8517 448.291,-160.3186\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"451.7911,-160.1307 448.291,-150.1308 444.7911,-160.1308 451.7911,-160.1307\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"transparent\" stroke=\"#000000\" d=\"M112.3737,-50C112.3737,-50 12.2083,-50 12.2083,-50 6.2083,-50 .2083,-44 .2083,-38 .2083,-38 .2083,-12 .2083,-12 .2083,-6 6.2083,0 12.2083,0 12.2083,0 112.3737,0 112.3737,0 118.3737,0 124.3737,-6 124.3737,-12 124.3737,-12 124.3737,-38 124.3737,-38 124.3737,-44 118.3737,-50 112.3737,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 21555.777</text>\n",
       "<text text-anchor=\"start\" x=\"12.6758\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3433</text>\n",
       "<text text-anchor=\"start\" x=\"12.2759\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 266.472</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M150.1758,-85.9947C136.1324,-76.2481 120.7288,-65.5575 106.794,-55.8864\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"108.5436,-52.8404 98.3327,-50.014 104.5525,-58.5911 108.5436,-52.8404\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.976471\" stroke=\"#000000\" d=\"M238.5354,-50C238.5354,-50 154.0466,-50 154.0466,-50 148.0466,-50 142.0466,-44 142.0466,-38 142.0466,-38 142.0466,-12 142.0466,-12 142.0466,-6 148.0466,0 154.0466,0 154.0466,0 238.5354,0 238.5354,0 244.5354,0 250.5354,-6 250.5354,-12 250.5354,-12 250.5354,-38 250.5354,-38 250.5354,-44 244.5354,-50 238.5354,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"157.5723\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 625.0</text>\n",
       "<text text-anchor=\"start\" x=\"158.355\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"150.1689\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 4925.0</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M196.291,-85.9947C196.291,-77.6273 196.291,-68.5643 196.291,-60.0478\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"199.7911,-60.014 196.291,-50.014 192.7911,-60.0141 199.7911,-60.014\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.019608\" stroke=\"#000000\" d=\"M372.3213,-50C372.3213,-50 280.2607,-50 280.2607,-50 274.2607,-50 268.2607,-44 268.2607,-38 268.2607,-38 268.2607,-12 268.2607,-12 268.2607,-6 274.2607,0 280.2607,0 280.2607,0 372.3213,0 372.3213,0 378.3213,0 384.3213,-6 384.3213,-12 384.3213,-12 384.3213,-38 384.3213,-38 384.3213,-44 378.3213,-50 372.3213,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"279.7861\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 60418.3</text>\n",
       "<text text-anchor=\"start\" x=\"276.6758\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2734</text>\n",
       "<text text-anchor=\"start\" x=\"276.2759\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 365.435</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M406.3055,-85.9947C393.6403,-76.3401 379.76,-65.7592 367.1681,-56.1604\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"369.1798,-53.293 359.1051,-50.014 364.9361,-58.86 369.1798,-53.293\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.062745\" stroke=\"#000000\" d=\"M514.3737,-50C514.3737,-50 414.2083,-50 414.2083,-50 408.2083,-50 402.2083,-44 402.2083,-38 402.2083,-38 402.2083,-12 402.2083,-12 402.2083,-6 408.2083,0 414.2083,0 414.2083,0 514.3737,0 514.3737,0 520.3737,0 526.3737,-6 526.3737,-12 526.3737,-12 526.3737,-38 526.3737,-38 526.3737,-44 520.3737,-50 514.3737,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"410\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 79330.053</text>\n",
       "<text text-anchor=\"start\" x=\"418.5688\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 365</text>\n",
       "<text text-anchor=\"start\" x=\"414.2759\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 563.425</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>6&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M453.7973,-85.9947C455.2368,-77.6273 456.7961,-68.5643 458.2613,-60.0478\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"461.7412,-60.4627 459.9875,-50.014 454.8426,-59.2758 461.7412,-60.4627\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.027451\" stroke=\"#000000\" d=\"M799.005,-150C799.005,-150 639.577,-150 639.577,-150 633.577,-150 627.577,-144 627.577,-138 627.577,-138 627.577,-98 627.577,-98 627.577,-92 633.577,-86 639.577,-86 639.577,-86 799.005,-86 799.005,-86 805.005,-86 811.005,-92 811.005,-98 811.005,-98 811.005,-138 811.005,-138 811.005,-144 805.005,-150 799.005,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"635.6841\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">bc_partner_OAF_low â‰¤ 8.5</text>\n",
       "<text text-anchor=\"start\" x=\"665\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 48261.916</text>\n",
       "<text text-anchor=\"start\" x=\"669.6758\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2137</text>\n",
       "<text text-anchor=\"start\" x=\"669.2759\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 389.132</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M789.8123,-185.8089C780.3486,-176.7092 769.9435,-166.7043 760.1,-157.2394\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"762.3413,-154.539 752.707,-150.1308 757.4895,-159.5848 762.3413,-154.539\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.062745\" stroke=\"#000000\" d=\"M1015.8228,-150C1015.8228,-150 840.7592,-150 840.7592,-150 834.7592,-150 828.7592,-144 828.7592,-138 828.7592,-138 828.7592,-98 828.7592,-98 828.7592,-92 834.7592,-86 840.7592,-86 840.7592,-86 1015.8228,-86 1015.8228,-86 1021.8228,-86 1027.8228,-92 1027.8228,-98 1027.8228,-98 1027.8228,-138 1027.8228,-138 1027.8228,-144 1021.8228,-150 1015.8228,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"836.5254\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">terms.repayment_term â‰¤ 16.5</text>\n",
       "<text text-anchor=\"start\" x=\"874.5161\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 81172.303</text>\n",
       "<text text-anchor=\"start\" x=\"878.6758\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3047</text>\n",
       "<text text-anchor=\"start\" x=\"878.2759\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 562.832</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>9&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M857.0916,-185.8089C866.6463,-176.7092 877.1515,-166.7043 887.0896,-157.2394\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"889.7261,-159.5618 894.5537,-150.1308 884.8985,-154.4928 889.7261,-159.5618\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.023529\" stroke=\"#000000\" d=\"M655.8416,-50C655.8416,-50 556.7404,-50 556.7404,-50 550.7404,-50 544.7404,-44 544.7404,-38 544.7404,-38 544.7404,-12 544.7404,-12 544.7404,-6 550.7404,0 556.7404,0 556.7404,0 655.8416,0 655.8416,0 661.8416,0 667.8416,-6 667.8416,-12 667.8416,-12 667.8416,-38 667.8416,-38 667.8416,-44 661.8416,-50 655.8416,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"552.5161\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 31191.403</text>\n",
       "<text text-anchor=\"start\" x=\"556.6758\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2087</text>\n",
       "<text text-anchor=\"start\" x=\"556.2759\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 369.789</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M680.4028,-85.9947C668.7837,-76.432 656.0604,-65.9606 644.4861,-56.4349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"646.6299,-53.6663 636.6844,-50.014 642.1816,-59.0712 646.6299,-53.6663\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.196078\" stroke=\"#000000\" d=\"M790.5871,-50C790.5871,-50 697.9949,-50 697.9949,-50 691.9949,-50 685.9949,-44 685.9949,-38 685.9949,-38 685.9949,-12 685.9949,-12 685.9949,-6 691.9949,0 697.9949,0 697.9949,0 790.5871,0 790.5871,0 796.5871,0 802.5871,-6 802.5871,-12 802.5871,-12 802.5871,-38 802.5871,-38 802.5871,-44 796.5871,-50 790.5871,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"693.8931\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 93325.25</text>\n",
       "<text text-anchor=\"start\" x=\"702.4619\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"698.6851\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 1196.5</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M727.8946,-85.9947C730.1686,-77.5354 732.6338,-68.365 734.945,-59.7672\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"738.3507,-60.5798 737.5668,-50.014 731.5907,-58.7626 738.3507,-60.5798\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.054902\" stroke=\"#000000\" d=\"M933.3737,-50C933.3737,-50 833.2083,-50 833.2083,-50 827.2083,-50 821.2083,-44 821.2083,-38 821.2083,-38 821.2083,-12 821.2083,-12 821.2083,-6 827.2083,0 833.2083,0 833.2083,0 933.3737,0 933.3737,0 939.3737,0 945.3737,-6 945.3737,-12 945.3737,-12 945.3737,-38 945.3737,-38 945.3737,-44 939.3737,-50 933.3737,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"829\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 68828.373</text>\n",
       "<text text-anchor=\"start\" x=\"833.6758\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2747</text>\n",
       "<text text-anchor=\"start\" x=\"833.2759\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 536.513</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M912.8046,-85.9947C908.6224,-77.3515 904.081,-67.966 899.8429,-59.2073\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"902.9008,-57.4912 895.3946,-50.014 896.5997,-60.5401 902.9008,-57.4912\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.113725\" stroke=\"#000000\" d=\"M1083.6597,-50C1083.6597,-50 974.9223,-50 974.9223,-50 968.9223,-50 962.9223,-44 962.9223,-38 962.9223,-38 962.9223,-12 962.9223,-12 962.9223,-6 968.9223,0 974.9223,0 974.9223,0 1083.6597,0 1083.6597,0 1089.6597,0 1095.6597,-6 1095.6597,-12 1095.6597,-12 1095.6597,-38 1095.6597,-38 1095.6597,-44 1089.6597,-50 1083.6597,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"971.1069\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 129776.972</text>\n",
       "<text text-anchor=\"start\" x=\"983.5688\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 300</text>\n",
       "<text text-anchor=\"start\" x=\"979.2759\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 803.833</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>13&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M963.0495,-85.9947C973.235,-76.6159 984.3699,-66.363 994.5541,-56.9855\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"997.1396,-59.3625 1002.1252,-50.014 992.398,-54.213 997.1396,-59.3625\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.572549\" stroke=\"#000000\" d=\"M1456.4671,-250C1456.4671,-250 1292.115,-250 1292.115,-250 1286.115,-250 1280.115,-244 1280.115,-238 1280.115,-238 1280.115,-198 1280.115,-198 1280.115,-192 1286.115,-186 1292.115,-186 1292.115,-186 1456.4671,-186 1456.4671,-186 1462.4671,-186 1468.4671,-192 1468.4671,-198 1468.4671,-198 1468.4671,-238 1468.4671,-238 1468.4671,-244 1462.4671,-250 1456.4671,-250\"/>\n",
       "<text text-anchor=\"start\" x=\"1287.9531\" y=\"-234.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">bc_partner_OAF_high â‰¤ 4.5</text>\n",
       "<text text-anchor=\"start\" x=\"1316.1069\" y=\"-220.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 321572.222</text>\n",
       "<text text-anchor=\"start\" x=\"1332.4619\" y=\"-206.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 15</text>\n",
       "<text text-anchor=\"start\" x=\"1320.3828\" y=\"-192.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 2996.667</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1374.291,-285.8089C1374.291,-277.6906 1374.291,-268.8517 1374.291,-260.3186\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1377.7911,-260.1307 1374.291,-250.1308 1370.7911,-260.1308 1377.7911,-260.1307\"/>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>24</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.894118\" stroke=\"#000000\" d=\"M1871.4671,-250C1871.4671,-250 1707.115,-250 1707.115,-250 1701.115,-250 1695.115,-244 1695.115,-238 1695.115,-238 1695.115,-198 1695.115,-198 1695.115,-192 1701.115,-186 1707.115,-186 1707.115,-186 1871.4671,-186 1871.4671,-186 1877.4671,-186 1883.4671,-192 1883.4671,-198 1883.4671,-198 1883.4671,-238 1883.4671,-238 1883.4671,-244 1877.4671,-250 1871.4671,-250\"/>\n",
       "<text text-anchor=\"start\" x=\"1702.9531\" y=\"-234.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">bc_partner_OAF_high â‰¤ 6.5</text>\n",
       "<text text-anchor=\"start\" x=\"1731.1069\" y=\"-220.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 354382.086</text>\n",
       "<text text-anchor=\"start\" x=\"1747.4619\" y=\"-206.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 21</text>\n",
       "<text text-anchor=\"start\" x=\"1735.3828\" y=\"-192.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 4538.095</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;24 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>16&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1468.7205,-295.2459C1533.1048,-279.7316 1618.3979,-259.1791 1684.8181,-243.1742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1685.9907,-246.4919 1694.8925,-240.7466 1684.3508,-239.6867 1685.9907,-246.4919\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.443137\" stroke=\"#000000\" d=\"M1355.6597,-150C1355.6597,-150 1246.9223,-150 1246.9223,-150 1240.9223,-150 1234.9223,-144 1234.9223,-138 1234.9223,-138 1234.9223,-98 1234.9223,-98 1234.9223,-92 1240.9223,-86 1246.9223,-86 1246.9223,-86 1355.6597,-86 1355.6597,-86 1361.6597,-86 1367.6597,-92 1367.6597,-98 1367.6597,-98 1367.6597,-138 1367.6597,-138 1367.6597,-144 1361.6597,-150 1355.6597,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"1256.9702\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">age_int â‰¤ 36.0</text>\n",
       "<text text-anchor=\"start\" x=\"1243.1069\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 137291.667</text>\n",
       "<text text-anchor=\"start\" x=\"1263.355\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"1255.1689\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 2375.0</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1350.7915,-185.8089C1344.4093,-177.0661 1337.417,-167.4876 1330.7504,-158.3553\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1333.4695,-156.144 1324.7465,-150.1308 1327.8157,-160.2713 1333.4695,-156.144\"/>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>21</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.658824\" stroke=\"#000000\" d=\"M1497.3737,-150C1497.3737,-150 1397.2083,-150 1397.2083,-150 1391.2083,-150 1385.2083,-144 1385.2083,-138 1385.2083,-138 1385.2083,-98 1385.2083,-98 1385.2083,-92 1391.2083,-86 1397.2083,-86 1397.2083,-86 1497.3737,-86 1497.3737,-86 1503.3737,-86 1509.3737,-92 1509.3737,-98 1509.3737,-98 1509.3737,-138 1509.3737,-138 1509.3737,-144 1503.3737,-150 1497.3737,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"1399.0874\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">num_tags â‰¤ 2.5</text>\n",
       "<text text-anchor=\"start\" x=\"1393\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 15015.432</text>\n",
       "<text text-anchor=\"start\" x=\"1409.355\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 9</text>\n",
       "<text text-anchor=\"start\" x=\"1394.9312\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 3411.111</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;21 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>17&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1397.7905,-185.8089C1404.1728,-177.0661 1411.1651,-167.4876 1417.8316,-158.3553\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1420.7663,-160.2713 1423.8356,-150.1308 1415.1125,-156.144 1420.7663,-160.2713\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.345098\" stroke=\"#000000\" d=\"M1210.8008,-50C1210.8008,-50 1125.7812,-50 1125.7812,-50 1119.7812,-50 1113.7812,-44 1113.7812,-38 1113.7812,-38 1113.7812,-12 1113.7812,-12 1113.7812,-6 1119.7812,0 1125.7812,0 1125.7812,0 1210.8008,0 1210.8008,0 1216.8008,0 1222.8008,-6 1222.8008,-12 1222.8008,-12 1222.8008,-38 1222.8008,-38 1222.8008,-44 1216.8008,-50 1210.8008,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"1121.7861\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 7656.25</text>\n",
       "<text text-anchor=\"start\" x=\"1130.355\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"1122.1689\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 1912.5</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1255.52,-85.9947C1241.5813,-76.2481 1226.2927,-65.5575 1212.4619,-55.8864\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1214.2647,-52.8762 1204.0638,-50.014 1210.2533,-58.6129 1214.2647,-52.8762\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>20</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.490196\" stroke=\"#000000\" d=\"M1353.3737,-50C1353.3737,-50 1253.2083,-50 1253.2083,-50 1247.2083,-50 1241.2083,-44 1241.2083,-38 1241.2083,-38 1241.2083,-12 1241.2083,-12 1241.2083,-6 1247.2083,0 1253.2083,0 1253.2083,0 1353.3737,0 1353.3737,0 1359.3737,0 1365.3737,-6 1365.3737,-12 1365.3737,-12 1365.3737,-38 1365.3737,-38 1365.3737,-44 1359.3737,-50 1353.3737,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"1249\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 41679.688</text>\n",
       "<text text-anchor=\"start\" x=\"1265.355\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 4</text>\n",
       "<text text-anchor=\"start\" x=\"1253.2759\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 2606.25</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;20 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>18&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1301.9793,-85.9947C1302.1592,-77.6273 1302.3541,-68.5643 1302.5373,-60.0478\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1306.0372,-60.087 1302.7531,-50.014 1299.0388,-59.9364 1306.0372,-60.087\"/>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>22</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.647059\" stroke=\"#000000\" d=\"M1495.1076,-50C1495.1076,-50 1395.4745,-50 1395.4745,-50 1389.4745,-50 1383.4745,-44 1383.4745,-38 1383.4745,-38 1383.4745,-12 1383.4745,-12 1383.4745,-6 1389.4745,0 1395.4745,0 1395.4745,0 1495.1076,0 1495.1076,0 1501.1076,0 1507.1076,-6 1507.1076,-12 1507.1076,-12 1507.1076,-38 1507.1076,-38 1507.1076,-44 1501.1076,-50 1495.1076,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"1394.8931\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 8420.139</text>\n",
       "<text text-anchor=\"start\" x=\"1407.355\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"1391.3828\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 3345.833</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;22 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>21&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1446.6027,-85.9947C1446.4228,-77.6273 1446.2279,-68.5643 1446.0447,-60.0478\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1449.5432,-59.9364 1445.829,-50.014 1442.5449,-60.087 1449.5432,-59.9364\"/>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>23</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.686275\" stroke=\"#000000\" d=\"M1637.1076,-50C1637.1076,-50 1537.4745,-50 1537.4745,-50 1531.4745,-50 1525.4745,-44 1525.4745,-38 1525.4745,-38 1525.4745,-12 1525.4745,-12 1525.4745,-6 1531.4745,0 1537.4745,0 1537.4745,0 1637.1076,0 1637.1076,0 1643.1076,0 1649.1076,-6 1649.1076,-12 1649.1076,-12 1649.1076,-38 1649.1076,-38 1649.1076,-44 1643.1076,-50 1637.1076,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"1536.8931\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 2638.889</text>\n",
       "<text text-anchor=\"start\" x=\"1549.355\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"1533.3828\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 3541.667</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;23 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>21&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1495.4711,-85.9947C1510.2817,-76.1562 1526.5404,-65.3558 1541.207,-55.613\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1543.2425,-58.4627 1549.6355,-50.014 1539.3692,-52.632 1543.2425,-58.4627\"/>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>25</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.796078\" stroke=\"#000000\" d=\"M1844.7842,-150C1844.7842,-150 1733.7979,-150 1733.7979,-150 1727.7979,-150 1721.7979,-144 1721.7979,-138 1721.7979,-138 1721.7979,-98 1721.7979,-98 1721.7979,-92 1727.7979,-86 1733.7979,-86 1733.7979,-86 1844.7842,-86 1844.7842,-86 1850.7842,-86 1856.7842,-92 1856.7842,-98 1856.7842,-98 1856.7842,-138 1856.7842,-138 1856.7842,-144 1850.7842,-150 1844.7842,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"1729.7944\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">days_to_fund â‰¤ 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"1731.1069\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 255941.358</text>\n",
       "<text text-anchor=\"start\" x=\"1751.355\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 9</text>\n",
       "<text text-anchor=\"start\" x=\"1735.3828\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 4069.444</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;25 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>24&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1789.291,-185.8089C1789.291,-177.6906 1789.291,-168.8517 1789.291,-160.3186\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1792.7911,-160.1307 1789.291,-150.1308 1785.7911,-160.1308 1792.7911,-160.1307\"/>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>28</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.968627\" stroke=\"#000000\" d=\"M2064.6597,-150C2064.6597,-150 1955.9223,-150 1955.9223,-150 1949.9223,-150 1943.9223,-144 1943.9223,-138 1943.9223,-138 1943.9223,-98 1943.9223,-98 1943.9223,-92 1949.9223,-86 1955.9223,-86 1955.9223,-86 2064.6597,-86 2064.6597,-86 2070.6597,-86 2076.6597,-92 2076.6597,-98 2076.6597,-98 2076.6597,-138 2076.6597,-138 2076.6597,-144 2070.6597,-150 2064.6597,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"1957.0288\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">children_int â‰¤ 6.5</text>\n",
       "<text text-anchor=\"start\" x=\"1952.1069\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 139943.576</text>\n",
       "<text text-anchor=\"start\" x=\"1968.4619\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 12</text>\n",
       "<text text-anchor=\"start\" x=\"1956.3828\" y=\"-92.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 4889.583</text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;28 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>24&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1860.1287,-185.9467C1883.9743,-175.1569 1910.661,-163.0814 1934.848,-152.1371\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1936.3165,-155.3143 1943.9843,-148.003 1933.4307,-148.9368 1936.3165,-155.3143\"/>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.925490\" stroke=\"#000000\" d=\"M1763.8008,-50C1763.8008,-50 1678.7812,-50 1678.7812,-50 1672.7812,-50 1666.7812,-44 1666.7812,-38 1666.7812,-38 1666.7812,-12 1666.7812,-12 1666.7812,-6 1672.7812,0 1678.7812,0 1678.7812,0 1763.8008,0 1763.8008,0 1769.8008,0 1775.8008,-6 1775.8008,-12 1775.8008,-12 1775.8008,-38 1775.8008,-38 1775.8008,-44 1769.8008,-50 1763.8008,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"1674.7861\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 62500.0</text>\n",
       "<text text-anchor=\"start\" x=\"1683.355\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"1675.1689\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 4675.0</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;26 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>25&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1765.8893,-85.9947C1759.3006,-76.9837 1752.122,-67.1658 1745.4878,-58.0927\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1748.3085,-56.0205 1739.5808,-50.014 1742.6579,-60.1522 1748.3085,-56.0205\"/>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.760784\" stroke=\"#000000\" d=\"M1914.6597,-50C1914.6597,-50 1805.9223,-50 1805.9223,-50 1799.9223,-50 1793.9223,-44 1793.9223,-38 1793.9223,-38 1793.9223,-12 1793.9223,-12 1793.9223,-6 1799.9223,0 1805.9223,0 1805.9223,0 1914.6597,0 1914.6597,0 1920.6597,0 1926.6597,-6 1926.6597,-12 1926.6597,-12 1926.6597,-38 1926.6597,-38 1926.6597,-44 1920.6597,-50 1914.6597,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"1802.1069\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 176505.102</text>\n",
       "<text text-anchor=\"start\" x=\"1822.355\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 7</text>\n",
       "<text text-anchor=\"start\" x=\"1806.3828\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 3896.429</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;27 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>25&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1813.7252,-85.9947C1820.6045,-76.9837 1828.0999,-67.1658 1835.0267,-58.0927\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1837.908,-60.0863 1841.1943,-50.014 1832.3441,-55.8386 1837.908,-60.0863\"/>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M2064.6597,-50C2064.6597,-50 1955.9223,-50 1955.9223,-50 1949.9223,-50 1943.9223,-44 1943.9223,-38 1943.9223,-38 1943.9223,-12 1943.9223,-12 1943.9223,-6 1949.9223,0 1955.9223,0 1955.9223,0 2064.6597,0 2064.6597,0 2070.6597,0 2076.6597,-6 2076.6597,-12 2076.6597,-12 2076.6597,-38 2076.6597,-38 2076.6597,-44 2070.6597,-50 2064.6597,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"1952.1069\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 131396.484</text>\n",
       "<text text-anchor=\"start\" x=\"1972.355\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 8</text>\n",
       "<text text-anchor=\"start\" x=\"1956.3828\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 5034.375</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;29 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>28&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2010.291,-85.9947C2010.291,-77.6273 2010.291,-68.5643 2010.291,-60.0478\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2013.7911,-60.014 2010.291,-50.014 2006.7911,-60.0141 2013.7911,-60.014\"/>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<path fill=\"#e58139\" fill-opacity=\"0.909804\" stroke=\"#000000\" d=\"M2191.8008,-50C2191.8008,-50 2106.7812,-50 2106.7812,-50 2100.7812,-50 2094.7812,-44 2094.7812,-38 2094.7812,-38 2094.7812,-12 2094.7812,-12 2094.7812,-6 2100.7812,0 2106.7812,0 2106.7812,0 2191.8008,0 2191.8008,0 2197.8008,0 2203.8008,-6 2203.8008,-12 2203.8008,-12 2203.8008,-38 2203.8008,-38 2203.8008,-44 2197.8008,-50 2191.8008,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"2102.7861\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">mse = 31250.0</text>\n",
       "<text text-anchor=\"start\" x=\"2111.355\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 4</text>\n",
       "<text text-anchor=\"start\" x=\"2103.1689\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = 4600.0</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;30 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>28&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2058.1269,-85.9947C2072.8318,-76.1562 2088.9744,-65.3558 2103.5362,-55.613\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2105.5394,-58.4838 2111.9045,-50.014 2101.6469,-52.6659 2105.5394,-58.4838\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x114fff6d8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(trained_regressor, out_file=None, \n",
    "                         feature_names=X_train.columns,  \n",
    "                         class_names=y_train.values,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even a DecisionTreeRegressor with a max depth of 4 is rather complicated.  To develop your intuition for the various input parameters, I recommend manually adjusting them up and down to see the impacts.\n",
    "\n",
    "If you were to tune the parameters manually and see how it impacts the training and test data sets that we created above, you would slowly tune towards a higher score on the test data set; however, you would most likely be overfitting to the test data set and the model would not generalize well to a secondary test data set.  To avoid this, it is recommended that you use [k-fold](https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/) validation.\n",
    "\n",
    "In addition to k-fold validation, we wil use sklearn's GridSearchCV, which allows us using k-fold validation to assess every permuation of possible values for the parameters that we provide. See the [Advanced Material](#AdvancedCV) at the bottom of this notebook for a quick overview of these two methods.\n",
    "\n",
    "**Note** since we are training one regressor one time for each possible permutation of specified parameter values, this next cell will take some time to run.  That is why you need to gain an intuition for which values to test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=42, splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [8, 10, 14], 'min_impurity_decrease': [0.1, 0.01, 0.0], 'min_samples_split': [10, 50, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'max_depth':[8, 10, 14], \n",
    "              'min_impurity_decrease': [.1,.01, 0.0],\n",
    "              'min_samples_split': [10, 50, 2]}\n",
    "decision_regressor= DecisionTreeRegressor(random_state=42)\n",
    "regressor = GridSearchCV(decision_regressor, parameters)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'mse',\n",
       " 'max_depth': 8,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.1,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 10,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'presort': False,\n",
       " 'random_state': 42,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have finished up training lets print out what GridSearchCV found to be the best parameters \n",
    "regressor.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see what happens when we train a DecisionTreeRegressor using these parameters on our original training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.6894\n",
      "Test score: 0.5692\n"
     ]
    }
   ],
   "source": [
    "trained_regressor = train_score_regressor(sklearn_regressor = DecisionTreeRegressor,\n",
    "                                          X_train = X_train, y_train = y_train, \n",
    "                                          X_test = X_test, y_test = y_test, \n",
    "                                          model_parameters = regressor.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance on the test data has increased from 0.43 to 0.56 - not bad!\n",
    "\n",
    "The R^2 number above is pretty telling but it is always nice to visualise how these look in a scattor plot. This allows us to visualise the variation between the predicted loan amount and the true loan amount values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # TODO: Replace with Seaborn\n",
    "# # plotting a graph of the true values vs the predicted values for the training and test datasets\n",
    "# def plot_y_yhat_scatter(y_actual,y_predicted,train_test):\n",
    "#     fig = plt.figure(figsize=(6, 6))\n",
    "#     plt.scatter(y_actual, y_predicted, alpha=0.5)\n",
    "#     plt.xlabel('True value')\n",
    "#     plt.ylabel('Predicted value')\n",
    "#     plt.title(\"relationship between True Y and predicted Y* loan amount ({0} dataset)\".format(train_test))\n",
    "#     plt.show()\n",
    "    \n",
    "# plot_y_yhat_scatter(y_train, regressor.predict(X_train),train_test = \"training\")\n",
    "# plot_y_yhat_scatter(y_test, regressor.predict(X_test),train_test = \"testing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bagging\n",
    "<a id='bagging'></a>\n",
    "\n",
    "As we just saw, Decision Trees naturally overfit to the data.  One approach to addressing this is by parameter tuning for an individual tree as we did above with the GridSearchCV algorithm. However, better perforamnce is generally achieved when an **ensemble approach** such as bagging is used.\n",
    "\n",
    "To start with Bagging is a shorter term for Bootstrapping Aggregation which is a general procedure to reduce variance in model that have a tendancy to overfit. Again, here is a [great blog](https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/) going into more detail if you need to refresh your memory. This diagram below is taken from the website to give a visual overview of how bagging work in the context of a classifying algorithm.\n",
    "\n",
    "<img src=\"images/bagging.png\",width=500,height=500>\n",
    "\n",
    "In the context of the decision tree, a bagging decision tree regressor does the following:\n",
    "- Creates N random subsample of the dataset using selection with replacement. (Statistically 1/3 of the data is left of each subsample)\n",
    "- Trains a new decision tree on each N subsample\n",
    "- Takes the prediction from each and every N tree and averages the result\n",
    "- This average is the final output of the bagging model. \n",
    "This is an **Ensemble approach** where we use the results of several random analyses to find an average response.\n",
    "\n",
    "Furthermore, for each tree, there will be a sample of data which was not used to train the algorithm. This means that we can also measure the **out-of-bag** score ( or oob_score). This score evaluates the performance of the model on the sample not used while training an individual tree. (Remember a random ~1/3 of data are not used to train a given tree).\n",
    "\n",
    "Now let's try this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9392\n",
      "Test score: 0.5955\n",
      "OOB score: 0.5302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda3/envs/good/lib/python3.6/site-packages/sklearn/ensemble/bagging.py:987: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initiating the bagging regressor algorithm\n",
    "trained_regressor = train_score_regressor(sklearn_regressor=BaggingRegressor,\n",
    "                                          X_train=X_train, y_train=y_train, \n",
    "                                          X_test=X_test, y_test=y_test, \n",
    "                                          model_parameters = {\"oob_score\":True, \n",
    "                                                              'random_state':42},\n",
    "                                          print_oob_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are very encouraging. Prior to any parameter tuning, the model is already preforming better on the test data set moving from 0.56 to 0.59. Also we now have access to the oob_score which also indicates how the model performs on unseen data. \n",
    "\n",
    "Therefore, when comparing different bagged regression models, we should looking at three main outputs:\n",
    "- Train score = R^2 between the predicte and true values for the training dataset\n",
    "- Test score = R^2 between the predicted and true values for the test dataset\n",
    "- Oob_score = the error rate of predicted values for data not used whilst training an individual tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing some model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default number of trees used in the bagging regressor is 10. This means that the algorithms split the dataset into 10 different tress, trains each one individually, predicts the result from each one individually and then averages the result. \n",
    "\n",
    "Let's see how performance changes when we increase the number of trees from 10 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9565\n",
      "Test score: 0.6135\n",
      "OOB score: 0.6867\n"
     ]
    }
   ],
   "source": [
    "trained_regressor = train_score_regressor(sklearn_regressor = BaggingRegressor,\n",
    "                                          X_train = X_train, y_train = y_train, \n",
    "                                          X_test = X_test, y_test = y_test, \n",
    "                                          model_parameters = {\"oob_score\":True, \"n_estimators\": 100,'random_state': 42},\n",
    "                                          print_oob_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see an improvement in all scores and in particular the oob_score. This is telling us that splitting the dataset into more tree leads to a more accurate average predicton of the loan_amount on unseen data - exactly what we are after!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # plotting a graph of the true values vs the predicted values for the training and test datasets\n",
    "# plot_y_yhat_scatter(y_train, trained_regressor.predict(X_train),train_test = \"training\")\n",
    "# # plotting a graph of the true values vs the predicted values for the training and test datasets\n",
    "# plot_y_yhat_scatter(y_test, trained_regressor.predict(X_test),train_test = \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "<a id='randomforest'></a>\n",
    "\n",
    "We still observe a large discrepancy between the training data and the test data.  This is largely because the decision trees are largely correlated, meaning that given subsets of the same data, the trees are probably going to split on the same features and result in very similar predictions.\n",
    "\n",
    "To de-correlate the trees, we can use a Random Forest Regressor, which at each split, only considers a random subsample of the features! This means that the trees are forced to make different split decisions. \n",
    "\n",
    "**The main difference between Bagged Decision Trees and Random Forest is that in the Random Forest only considers a random subset of the features at each split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features = 30\n"
     ]
    }
   ],
   "source": [
    "# Check how many columns our dataset has...\n",
    "print(f'number of features = {X_train.shape[1]}')\n",
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets restrict the numbers that the trees can choose from at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9573\n",
      "Test score: 0.6246\n",
      "OOB score: 0.6906\n"
     ]
    }
   ],
   "source": [
    "trained_regressor = train_score_regressor(sklearn_regressor=RandomForestRegressor,\n",
    "                                          X_train=X_train, y_train=y_train, \n",
    "                                          X_test=X_test, y_test=y_test, \n",
    "                                          model_parameters = {\"oob_score\": True, \n",
    "                                                              \"n_estimators\": 100, \n",
    "                                                              \"max_features\": num_features//2, \n",
    "                                                              'random_state': 42},\n",
    "                                          print_oob_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the random forest algorithm has only  slightly improved the performance relative to the bagging regressor. This indicates that the features we are using are already highly uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's consider some other hyperparameters. We can set the max_depth to 15 to improve upon overfitting, increase the number of trees and limited the maximum number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8918\n",
      "Test score: 0.6279\n",
      "OOB score: 0.6962\n"
     ]
    }
   ],
   "source": [
    "trained_regressor = train_score_regressor(sklearn_regressor= RandomForestRegressor,\n",
    "                                          X_train = X_train, y_train = y_train, \n",
    "                                          X_test = X_test, y_test = y_test, \n",
    "                                          model_parameters = {\"oob_score\":True, \n",
    "                                                              \"n_estimators\": 300,\n",
    "                                                              'random_state': 42, \n",
    "                                                              'max_depth': 15,\n",
    "                                                              'max_features': 15},\n",
    "                                          print_oob_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=15,\n",
       "           max_features=15, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=100, n_jobs=-1, oob_score=True, random_state=42,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_regressor = RandomForestRegressor(n_estimators=100,\n",
    "                                     max_depth=15, \n",
    "                                     max_features=15, \n",
    "                                     n_jobs= -1,\n",
    "                                     oob_score=True,\n",
    "                                     random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the oob_score has increased slightly - this is what we expect given that the number of estimators (trees) has increased. In general, increasing the number of trees increases performance until it plateaus. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, how do we know the best combination of hyperparameters? Cross validated Grid Search! Let's try it out with our random forest algorithm -  we'll just search over a small range to same time but usually we would search over a bigger range of hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [7, 10, 15], 'max_features': ['sqrt', 15], 'n_estimators': [300, 400]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'max_depth': [7,10,15], \n",
    "              'max_features': ['sqrt', 15],\n",
    "              'n_estimators': [300,400]}\n",
    "rf_regressor= RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "regressor = GridSearchCV(rf_regressor, parameters)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': 15,\n",
       " 'max_features': 15,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 400,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': False,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have finished up training lets print out what GridSearchCV found to be the best parameters \n",
    "regressor.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-7b732d3a64de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                           \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                           \u001b[0mmodel_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                           print_oob_score=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-ce41e072f558>\u001b[0m in \u001b[0;36mtrain_score_regressor\u001b[0;34m(sklearn_regressor, X_train, y_train, X_test, y_test, model_parameters, print_oob_score)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Step 2: Training the algorithm using the X_train dataset of features and y_train, the associated target features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Step 3: Calculating the score of the predictive power on the training and testing dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/good/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 327\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/good/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/good/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/good/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/good/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/good/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/good/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = regressor.best_estimator_.get_params()\n",
    "params['oob_score'] = True\n",
    "trained_regressor = train_score_regressor(sklearn_regressor=RandomForestRegressor,\n",
    "                                          X_train=X_train, y_train=y_train, \n",
    "                                          X_test=X_test, y_test=y_test, \n",
    "                                          model_parameters=params,\n",
    "                                          print_oob_score=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance\n",
    "<a id='featureimportance'></a>\n",
    "\n",
    "Now, let's try to figure out which features are driving our model's predictions.  Although we do not have the coefficients provided by a linear regression, we can analyze what percentage of the overall variance was explained by a given feature. This is known as feature importance. An important but counter-intuitive thing to note here is that the magnitude of the 'importance' is not indicative of how important the feature is, only the order matters!\n",
    "\n",
    "For example,\n",
    "- feature A has an importance of 0.5 \n",
    "- feature B has an importance of 0.25. \n",
    "\n",
    "Then all we can take away is that feature A is more explains more variance then feautre B, **not** that feature A explains twice as much as feature B. \n",
    "\n",
    "Let's look at this for our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking:\n",
      "1. feature bc_partner_OAF_high (0.270266)\n",
      "2. feature partner_dollar_amount (0.125090)\n",
      "3. feature terms.repayment_term (0.118754)\n",
      "4. feature days_to_fund (0.099321)\n",
      "5. feature posted_month (0.046954)\n",
      "6. feature hours_to_fund (0.038022)\n",
      "7. feature age_int (0.035121)\n",
      "8. feature posted_year (0.033323)\n",
      "9. feature partner_delinquency_rate (0.030932)\n",
      "10. feature num_partner_countries (0.023525)\n",
      "11. feature bc_partner_OAF_low (0.021375)\n",
      "12. feature children_int (0.020711)\n",
      "13. feature top_partner_id (0.020160)\n",
      "14. feature more_one_partner_country (0.017748)\n",
      "15. feature num_tags (0.013604)\n",
      "16. feature bc_partner_HIHEA (0.013316)\n",
      "17. feature exploratory_partner (0.012949)\n",
      "18. feature pct_female (0.010272)\n",
      "19. feature sector_Agriculture (0.009287)\n",
      "20. feature married (0.006177)\n",
      "21. feature tag_#Repeat Borrower (0.006069)\n",
      "22. feature bc_partner_others (0.005780)\n",
      "23. feature female (0.004198)\n",
      "24. feature tag_#Schooling (0.004045)\n",
      "25. feature sector_Personal Use (0.003155)\n",
      "26. feature tag_#Woman Owned Biz (0.003014)\n",
      "27. feature kids (0.002822)\n",
      "28. feature parent (0.002739)\n",
      "29. feature sector_Health (0.000870)\n",
      "30. feature sector_Wholesale (0.000401)\n"
     ]
    }
   ],
   "source": [
    "# Get the feature importances from our final trained model...\n",
    "importances = trained_regressor.feature_importances_\n",
    "\n",
    "# The feature importances are given in the order the features are given in the dataset\n",
    "\n",
    "# Lets find the indices of the feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the important features in descending order\n",
    "print('Feature Ranking:')\n",
    "for f in range(len(importances)):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, X_train.columns[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is interesting here is that there is not a very clear relationship between any single feature and the loan_amount. However the aggregat these features together into either the bagged regressor or the random forest algorithm leads to very effective predictions (R^2 ~ 0.62). This is a testament to the predictive power of decisions trees and ensemble methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try taking the top 20 features and observe it's impact on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_cols = [X_train.columns[indices[f]] for f in range(20)]\n",
    "X_train2 = X_train[relevant_cols]\n",
    "X_test2 = X_test[relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9012\n",
      "Test score: 0.6191\n",
      "OOB score: 0.6948\n"
     ]
    }
   ],
   "source": [
    "trained_regressor = train_score_regressor(sklearn_regressor=RandomForestRegressor,\n",
    "                                          X_train=X_train2, y_train=y_train, \n",
    "                                          X_test=X_test2, y_test=y_test, \n",
    "                                          model_parameters=trained_regressor.get_params(),\n",
    "                                          print_oob_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores haven't changed significantly which means that we can continue just using the top 20 features. This means the algorithms is much faster and gives us a clearer idea of which features affect the loan amount\n",
    "\n",
    "Also, if we reflect on our previous laboratory and compare the results to our final linear regression model, we see that we have improved the test_data R2 from 0.54 (linear regression) to 0.61 (random forest)! Not bad!\n",
    "\n",
    "Lastly, lets look at the root mean square of the predictions. Recall that the mean squared error which tells us how much error our model produced on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root mean squared error on test data: 209.0\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the test data\n",
    "y_pred_test = trained_regressor.predict(X_test2)\n",
    "print(f'root mean squared error on test data: {np.sqrt(mean_squared_error(y_test,y_pred_test)):.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see an improvement from 273 (linear regression) to 210 (random forest)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace with Seaborn\n",
    "# # plotting a grpah of the true values vs the predicted values for the training and test datasets\n",
    "# plot_y_yhat_scatter(y_train, trained_regressor.predict(X_train2),train_test = \"training\")\n",
    "# # plotting a grpah of the true values vs the predicted values for the training and test datasets\n",
    "# plot_y_yhat_scatter(y_test, trained_regressor.predict(X_test2),train_test = \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move onto homework, remember to output your final dataframe so you don't have to rerun it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.to_csv(data_path+'df_module_3-2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Days to Fund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the BaggingRegressor and the RandomForestRegressor method with a new target feature: days_to_fund. Determine the monst important features and discuss which features are intuitive or a surprise and why. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['days_to_fund']\n",
    "# drop returns a copy of the DataFrame with the specified columns removed.  \n",
    "X = cols.drop('days_to_fund', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create X_train, y_train, X_test and y_test where the y value is the days_to_fund feature. \n",
    "# Try using test_train_split function imported from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the three basic steps:\n",
    "- step 1: Initiate the algorithm\n",
    "- step 2: Fit the algorithm to the training data\n",
    "- step 3: Evaluate the algorithm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the BaggingRegressor method to predict the days_to_fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the RandomForest Regressor method to predict the days_to_fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Homework (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn RandomForest library also has a classifier method, which rather than predicting a continuous numerical value such as loan amount or days to fund, it attempted to classify data on existing labels, eg: male or female. \n",
    "\n",
    "We imported this method at the beginning and you can read more about it here using the following cell or by going the sklearn [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `RandomForestClassifier` not found.\n"
     ]
    }
   ],
   "source": [
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this module and the original feature gender, see how well we can classify loans by the gender. The procedure for training and testing data is practically the same as the RandomForestRegressor and the score is the mean accuracy of the predictions rather than the R^2 of the regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Material: Optimising the algorithm\n",
    "<a id='AdvancedCV'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-folds example for finding optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, lets look at how the training and test scores change as we change the depth of the tree..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depth_range = [2,4,6,8,10,12,14,16,18,20,22,24]\n",
    "scores_train = np.zeros(len(depth_range))\n",
    "scores_test = np.zeros(len(depth_range))\n",
    "for i in range(len(depth_range)):\n",
    "    dt_regressor = DecisionTreeRegressor(max_depth=depth_range[i], random_state=42)\n",
    "    model = dt_regressor.fit(X_train, y_train)\n",
    "    scores_train[i] = model.score(X_train, y_train)\n",
    "    scores_test[i] = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # TODO: Replace with Seaborn\n",
    "# plt.plot(depth_range, scores_train, label='training score')\n",
    "# plt.plot(depth_range, scores_test, label='testing score')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the depth:\n",
    "- The training score increases\n",
    "- But the testing score decreases\n",
    "\n",
    "Once the test score starts decreasing, this indicates that the model is overfitting. \n",
    "\n",
    "Here we could be tempted to say that the optimal depth is approximately 8 as this corresponds to the maximum score for the testing data. **However this is not the case**. The test set is just random fixed subset of data so choosing the optimal parameter here would be overfitting to the testset. \n",
    "\n",
    "This is where K-Folds cross validation comes in! This method does the following:\n",
    "- Splits the dataset K equal random subsests\n",
    "- Trains the data on K-1 subsets\n",
    "- Evaluates performance on Kth left out subset\n",
    "- Stores evaluation metric\n",
    "- Repeats for K times for each random subset\n",
    "\n",
    "Therefore if K = 5, the algorithm trains 5 times. Each time it holds out a 5th of the data, trains on the other 4/5ths and then evaluates the performance on the held out 5th. \n",
    "\n",
    "Here is an example of how the cross validation score changes with maximum tree depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_cv = np.zeros(len(depth_range))\n",
    "for i in range(len(depth_range)):\n",
    "    dt_regressor = DecisionTreeRegressor(max_depth=depth_range[i], random_state=42)\n",
    "    cv_scores = cross_val_score(dt_regressor, X_train, y_train,cv=5, n_jobs=-1)\n",
    "    scores_cv[i] = np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # TODO: Replace with Seaborn\n",
    "# plt.plot(depth_range, scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see the same general trend of the score increasing intially and then dropping off. However, the changes are less pronounced than above in the test dataset as we are generalising by using cross-validation. \n",
    "\n",
    "From this curve, the optimal max_depth would be 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV (CV = cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we were looking at a single parameter. However, to increase perfromance we should try adjusting several parameters. \n",
    "\n",
    "Sklearn's GridSearchCV uses the cross-validation bove to assess the performance of each possible permutation of the hyper-parameters that you specify. It then returns a model initialised with the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GridSearchCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': [100, 300, 500],\n",
    "             'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth': [None, 5, 10]}\n",
    "gridrf = RandomForestRegressor(n_jobs=-1)\n",
    "grid_rf = GridSearchCV(gridrf, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [100, 300, 500], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [None, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out which set of parameters performed best, we can use the .best\\_pramas\\_ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'max_features': 'auto', 'n_estimators': 300}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's check out the variation in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.676 (+/-0.027) for {'max_depth': None, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.680 (+/-0.029) for {'max_depth': None, 'max_features': 'auto', 'n_estimators': 300}\n",
      "0.679 (+/-0.030) for {'max_depth': None, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.660 (+/-0.048) for {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.661 (+/-0.040) for {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "0.663 (+/-0.041) for {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.651 (+/-0.047) for {'max_depth': None, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.657 (+/-0.043) for {'max_depth': None, 'max_features': 'log2', 'n_estimators': 300}\n",
      "0.657 (+/-0.047) for {'max_depth': None, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.588 (+/-0.040) for {'max_depth': 5, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.592 (+/-0.043) for {'max_depth': 5, 'max_features': 'auto', 'n_estimators': 300}\n",
      "0.589 (+/-0.043) for {'max_depth': 5, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.533 (+/-0.059) for {'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.539 (+/-0.067) for {'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "0.538 (+/-0.057) for {'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.515 (+/-0.062) for {'max_depth': 5, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.516 (+/-0.059) for {'max_depth': 5, 'max_features': 'log2', 'n_estimators': 300}\n",
      "0.520 (+/-0.066) for {'max_depth': 5, 'max_features': 'log2', 'n_estimators': 500}\n",
      "0.674 (+/-0.031) for {'max_depth': 10, 'max_features': 'auto', 'n_estimators': 100}\n",
      "0.674 (+/-0.031) for {'max_depth': 10, 'max_features': 'auto', 'n_estimators': 300}\n",
      "0.675 (+/-0.031) for {'max_depth': 10, 'max_features': 'auto', 'n_estimators': 500}\n",
      "0.636 (+/-0.048) for {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "0.639 (+/-0.040) for {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "0.637 (+/-0.042) for {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 500}\n",
      "0.622 (+/-0.044) for {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100}\n",
      "0.623 (+/-0.047) for {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300}\n",
      "0.621 (+/-0.046) for {'max_depth': 10, 'max_features': 'log2', 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "means = grid_rf.cv_results_['mean_test_score']\n",
    "stds = grid_rf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_rf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is also worth mentioning that that Decision Trees can also be used to classify data.\n",
    "\n",
    "For example some interesting classification questions we could investigate are:\n",
    "\n",
    "- Can we classify which loans expired and which one got funded?\n",
    "- Is a loan posted by a male or female?\n",
    "\n",
    "Here is a very simple example of how the Random Forest Classifier works: https://bicorner.com/2015/10/26/random-forest-using-ipython/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
